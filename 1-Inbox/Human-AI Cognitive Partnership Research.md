# **Cognitive Sovereignty and the Artificial Mind: A Comprehensive Report on Human-AI Collaboration, Augmentation, and Psychological Transformation**

## **Executive Summary**

The integration of Artificial Intelligence (AI)—specifically large language models (LLMs) and generative adversarial networks (GANs)—into the daily cognitive and creative workflows of individuals represents a rupture in the traditional ontology of human-tool interaction. We are currently witnessing a transition from **instrumental use**, where tools are passive extensions of human intent, to **systemic integration**, where the tool functions as an active, stochastic, and semi-autonomous cognitive partner. This report, "Cognitive Sovereignty and the Artificial Mind," provides an exhaustive analysis of the psychological, cognitive, and philosophical dimensions of this emerging partnership, drawing upon scholarship from 2018 to the present.

The research identifies a fundamental paradox at the heart of the human-AI relationship: the tension between **augmentation** (the expansion of cognitive capacity through scaffolding) and **atrophy** (the erosion of internal schemas due to excessive cognitive offloading). Current literature suggests that while AI offers unprecedented opportunities for "hybrid intelligence"—where the combined human-AI unit outperforms either entity alone—it also poses significant risks to "epistemic agency," defined as the capacity and willingness of the individual to take responsibility for their own beliefs and knowledge claims.

Unlike previous cognitive technologies such as the calculator or the GPS, Generative AI is characterized by **generativity** and **stochasticity**. It does not merely retrieve information or execute deterministic calculations; it engages in probabilistic reasoning and semantic generation, creating "branching points" in cognition where the human role shifts from creator to curator. This structural difference necessitates new theoretical frameworks, moving beyond traditional *Distributed Cognition* to models of *Calibrated Sovereignty* and *Material Engagement*.

Empirical findings from 2020-2025 reveal a complex landscape. While "cyborg" developers and writers experience heightened flow states and productivity, there is a measurable "dose-response" relationship between uncritical AI reliance and the degradation of critical thinking skills. The phenomenon of the "Hollowed Mind"—where the external system becomes indispensable for basic cognitive functioning—contrasts with the ideal of the "Fortified Mind," where AI serves as a "sparring partner" to strengthen internal capabilities.

This report establishes that the future of human intelligence depends not on competing with the machine's generative speed, but on cultivating distinctively human meta-skills: **evaluative judgment, mission-locking, and systemic integration**. The following sections detail the theoretical underpinnings, historical analogies, emerging empirical realities, and developmental implications of this profound cognitive shift.

## ---

**1\. Theoretical Frameworks: From Tools to Partners**

To understand the psychological impact of AI, researchers have had to retrieve, adapt, and dramatically expand cognitive theories that were originally developed to explain static tools. The complexity of Human-AI Interaction (HAI) forces a re-evaluation of the boundary between the biological "self" and the technological "other." The literature indicates that we are moving beyond simple "interaction" into a phase of "entanglement," requiring frameworks that can account for the agency, however simulated, of the non-biological partner.

### **1.1 The Extended Mind Thesis (EMT) in the Age of Algorithms**

The Extended Mind Thesis (EMT), originally proposed by Clark and Chalmers (1998), argues that cognition does not stop at the skull but spills out into the environment, incorporating tools—such as notebooks, smartphones, and now AI—as constitutive parts of the cognitive process.1 In this view, the "mind" is a hybrid system comprised of biological and non-biological elements.

#### **1.1.1 Active Externalism and the "System 0"**

In the context of Generative AI, EMT is being radically scaled. Traditional EMT focused on passive externalism (e.g., consulting a notebook). Generative AI introduces **active externalism**. When a user engages in a dialogue with an LLM, the system does not just store information; it processes, synthesizes, and reframes it. Scholarship suggests that AI is functioning as a "System 0"—an external cognitive layer that pre-processes information before it even reaches the user's "System 1" (intuition) or "System 2" (analytical reasoning). This creates a "hybrid cognitive unit" where the boundaries of authorship and ideation become porous.1

#### **1.1.2 The Transparency Condition and "Hostile" Collaboration**

Classically, for a tool to be considered an extended mind, it must be transparent (used without conscious effort) and reliable. Generative AI challenges this paradigm directly. Its tendency toward "hallucinations" (fabrications) and its stochastic nature violate the reliability criterion, forcing the user into a constant state of meta-cognitive monitoring. This suggests that AI functions less like a transparent "memory drive" and more like a "hostile collaborator" or a "junior partner" that requires constant supervision.2 The "extension" is not seamless; it is negotiated. This friction is where the cognitive work—and the potential for either growth or fatigue—resides.

#### **1.1.3 The Hollowed Mind Hypothesis**

Recent theoretical expansions warn of a dark side to EMT. If the extension does too much of the heavy lifting, the biological core may degrade. The **Hollowed Mind Hypothesis** suggests that while the *system* (Human+AI) is smarter and more capable, the *human* component becomes less capable independent of the system. This is a critical theoretical distinction from traditional EMT, which largely viewed extension as a neutral or positive expansion of capacity. The "Hollowed Mind" posits that deep learning requires "constructive friction" and memory consolidation, processes that are bypassed when AI performs the synthesis and structuring of information.4

### **1.2 Distributed Cognition (DCog) and Calibrated Sovereignty**

Distributed Cognition views knowledge as propagating through a system of people and artifacts. Hutchins’ seminal work on ship navigation is now being reapplied to "Cyborg Developers" and AI-assisted teams, but with significant modifications to account for the AI's adaptability.1

#### **1.2.1 Dynamic vs. Static Distribution**

Traditional DCog studies focused on static tool configurations (maps, compasses, cockpit displays) where the distribution of labor was architecturally fixed. In contrast, Human-AI collaboration involves **adaptive systems** where the distribution is dynamically negotiated in real-time. A software developer using an AI assistant does not have a fixed workflow; the AI might suggest a block of code, the human might reject it, the AI might offer an alternative, and the human might then edit that alternative. This "dynamic negotiation" of cognitive load requires a new conceptual vocabulary, as the "artifact" is effectively participating in the decision-making loop.1

#### **1.2.2 Implicit Social Forcefields**

Emerging research indicates that AI acts as a socially influential actor within distributed systems. In human teams, the presence of an AI "partner" reshapes linguistic and social coordination. Studies show that AI systems function as **implicit social forcefields**: their presence can erode epistemic diversity by converging the team on the AI's "median" logic. The AI's outputs, often polished and fluent, can anchor group discussions, narrowing the window of exploration and potentially undermining natural alignment processes. This suggests that AI is not a neutral tool but a "socially influential actor" that reorganizes the distributed cognitive architecture of the team.7

#### **1.2.3 Calibrated Sovereignty**

To counter the potential loss of control in these distributed systems, the framework of **Calibrated Sovereignty** is emerging. This theoretical stance posits that healthy distribution requires the human to maintain "mission-locking"—the ability to define the goal state and veto the AI's trajectory. Calibrated sovereignty emphasizes that while the *process* of cognition is distributed, the *responsibility* for the outcome must remain centralized in the human agent. This requires the development of specific "meta-skills" to manage the AI's output, ensuring that the distribution of labor does not lead to a distribution (and dilution) of agency.4

### **1.3 Material Engagement Theory (MET) and "Thinging"**

Material Engagement Theory (MET), championed by Malafouris, argues that thinking is not just a mental process that happens *before* action, but a process that happens *through* interaction with materials. This offers a compelling lens for understanding creative work with AI.11

#### **1.3.1 The Dance of Agency**

In MET, the artist does not impose a pre-formed idea onto passive clay; the idea emerges from the resistance and affordances of the clay—a process called "thinging." With AI, the "material" is hyper-responsive, semantic, and semi-autonomous. The "resistance" comes not from physical properties, but from prompt adherence, unexpected outputs, and the stochastic nature of the model. This makes the creative process a **dance of agency** where the human thinks *through* the AI's generation. The AI is not just a tool for executing an idea; it is a partner in the *discovery* of the idea. The artist engages in curation and contextual framing, harnessing the model's generative potential to reach a coherent expression that neither could have achieved alone.12

### **1.4 Transactive Memory Systems (TMS)**

TMS theory explains how groups divide memory labor ("I remember the math; you remember the history"). In the age of AI, the partner in this system is non-human, leading to significant shifts in how memory is valued and managed.14

#### **1.4.1 The Super-Partner and the Knowledge Collapse**

Traditionally, TMS relies on knowing *who* knows *what*. With AI, the assumption becomes "The AI knows *everything*." This leads to a profound scaling of the **Google Effect**, where humans cease encoding information entirely, trusting the external repository. However, unlike a static encyclopedia, the AI synthesizes and often hallucinates information. If the human partner in the TMS abdicates their role as a verifier, the system becomes fragile. The "transactive" nature is lost, replaced by a unidirectional dependency where the human becomes a consumer rather than a co-holder of knowledge.14

## ---

**2\. Historical and Analogical Precedents**

To determine if the current anxiety regarding AI-induced "skill atrophy" is valid, it is necessary to examine historical technologies that externalized human cognition. While history does not repeat itself, it rhymes, and the patterns of adaptation to previous cognitive technologies offer crucial insights into the present transition.

### **2.1 Writing: The First Cognitive Offloading**

The original debate on cognitive technology appears in Plato’s *Phaedrus*, where Socrates argues that writing is a *pharmakon*—both a cure and a poison—that will destroy memory and create the appearance of wisdom without the reality.18

#### **2.1.1 Cognitive Restructuring vs. Atrophy**

Socrates was, in a sense, correct: writing did atrophy oral memory traditions. We no longer possess the capacity to memorize epic poems like the *Iliad* after a single hearing. However, this atrophy in *storage* capacity allowed for a massive expansion in *analytic* capacity. Writing allowed complex arguments to be externalized, critiqued, and refined over time, enabling the development of philosophy, law, and science. The key distinction is that writing externalized the *storage* and *transmission* of thought, but the *synthesis* of the thought remained a purely human cognitive act.

#### **2.1.2 The AI Divergence**

The parallel with AI breaks down at the point of synthesis. Writing preserves the thought you had; AI suggests thoughts you *didn't* have. Writing is a passive externalization; AI is **active generation**. This suggests that the "atrophy" risk of AI might be significantly higher because it bypasses the *synthesis* phase of cognition, not just the *storage* phase. If writing restructured the mind by allowing us to offload memory, AI threatens to restructure the mind by allowing us to offload thinking itself.19

### **2.2 The Great Calculator Debate**

The introduction of electronic calculators in education (1970s-80s) sparked intense fears that students would lose basic arithmetic skills, serving as the most common analogy for AI in the classroom today.21

#### **2.2.1 The "Crutch" Argument vs. Conceptual Growth**

Research from the "Math Wars" era generally found that while mental calculation speed might decrease with calculator use, conceptual understanding of mathematics could increase if the calculator was used to bypass rote computation to focus on higher-order problem-solving. The calculator acted as a scaffold, allowing students to access more complex problems without being bogged down by arithmetic mechanics.21

#### **2.2.2 Determinism vs. Probabilistic Reasoning**

However, the calculator analogy is critically limited when applied to AI. A calculator is **deterministic**: ![][image1] will always equal ![][image2]. It is a tool for verification and speed, and its output is unambiguous. AI, by contrast, is **probabilistic** and **semantic**. Using a calculator does not relieve the student of formulating the equation; using an LLM to "write an essay" or "solve a word problem" relieves the student of formulating the *argument* or the *logic*. The calculator automates the *execution*; AI automates the *formulation*. Therefore, the risk of "conceptual atrophy" is structurally higher with AI than with calculators.24

### **2.3 GPS and the Hippocampus**

Navigation technologies offer the most biologically grounded precedent for cognitive atrophy, providing a stark warning for the AI era.27

#### **2.3.1 Use It or Lose It: The Neurological Evidence**

Extensive studies on London taxi drivers (who memorize "The Knowledge") versus GPS users show that reliance on turn-by-turn navigation leads to reduced gray matter density in the hippocampus, the brain region responsible for spatial memory and mapping. This is a direct biological feedback loop: the brain reallocates energy away from circuits that are being outsourced to technology.27

#### **2.3.2 The "Cognitive GPS"**

If GPS atrophy is a physical reality for spatial cognition, it stands to reason that "Cognitive GPS" (AI planning, writing, and structuring) could lead to similar atrophy in the neural circuits responsible for narrative structuring, logical sequencing, and critical analysis. The "Hollowed Mind" is not just a metaphor; it is a potential neurological outcome. If we stop building "mental maps" of our arguments because the AI provides the "turn-by-turn" directions for our essays, we risk losing the ability to navigate the landscape of ideas independently.30

### **2.4 Spell Checkers and Grammar Tools**

Spell checkers represent an intermediate step between calculators and AI, operating on language but at a surface level.32

#### **2.4.1 Surface Correction vs. Literacy**

Research indicates that while spell checkers produce polished final products, they can mask underlying deficits in literacy. Over-reliance can lead to an inability to self-edit or recognize errors without the tool. However, historically, these tools did not replace the *content* generation, only the *surface* correction. Generative AI crosses this line by generating the content itself, moving from "form correction" to "idea generation," thereby impacting a much deeper layer of cognitive processing.34

| Technology | Cognitive Function Offloaded | Psychological Outcome | Relevance to AI |
| :---- | :---- | :---- | :---- |
| **Writing** | Long-term Memory / Oral Tradition | Analytic expansion; loss of rote memory. | **High:** Precedent for externalizing thought, but AI adds *generation*. |
| **Calculator** | Arithmetic Computation | Focus shift to problem-solving; loss of mental math speed. | **Medium:** AI is probabilistic, not deterministic like calculators. |
| **GPS** | Spatial Mapping / Navigation | Hippocampal atrophy; loss of spatial awareness. | **Very High:** Biological proof of "use it or lose it" for cognitive maps. |
| **Spell Check** | Orthography / Grammar | Polished output; potential masking of literacy gaps. | **Medium:** Precursor to AI's ability to mask incompetence. |
| **Generative AI** | Synthesis / Reasoning / Ideation | **Unknown / Emerging:** Potential for "Epistemic Atrophy." | **N/A** (The subject of study). |

*Table 1: Comparative Analysis of Cognitive Technologies and Atrophy Risks.*

## ---

**3\. Structural Features of AI as a Cognitive Partner**

To understand why AI affects psychology differently than previous tools, we must isolate its unique ontological and structural features. It is these features—specifically its probabilistic nature and semantic fluency—that trigger specific psychological responses in the user.

### **3.1 Generativity and Stochasticity**

Unlike the deterministic tools of the past (typewriters, calculators, spreadsheets), Generative AI is **stochastic**. It operates on probabilities, meaning its outputs are variable, creative, and occasionally erroneous ("hallucinations").

#### **3.1.1 Branching Points in Cognition**

This feature forces the user into a new cognitive mode. Instead of linear production, the user faces **branching points** where the AI suggests multiple trajectories. The cognitive load shifts from *generating* the next step to *evaluating* the AI's proposed steps. This requires "evaluative judgement" rather than "generative capacity." The stochasticity amplifies the "exploration" phase of creativity but complicates the "execution" phase, as the user must constantly prune the probabilistic tree of the AI's output.36

#### **3.1.2 Hallucination as Feature and Bug**

Philosophically, AI "hallucination" is reframed in some literature as "digital dreaming" or a window into the probabilistic nature of creativity itself. However, practically, it creates a "trust paradox." The user is working with a partner that is incredibly knowledgeable but also prone to confident fabrication. This forces the user to maintain a state of "suspicious cooperation," a psychological state that is cognitively taxing and distinct from the blind trust we place in a calculator.3

### **3.2 Natural Language Interface and Anthropomorphism**

The primary interface of GenAI is natural language, which triggers deep-seated social heuristics in the human brain. We are evolutionarily wired to perceive anything that speaks fluently as a sentient agent.

#### **3.2.1 The ELIZA Effect 2.0 and Social Presence**

Users inevitably anthropomorphize agents that speak fluently, interpreting them as entities with social presence, empathy, and personality. This is the **ELIZA Effect** (referencing the 1960s chatbot) but amplified by the sophistication of LLMs. This social presence fosters **emotional dependence**, leading users to treat the AI as a companion or therapist. It lowers the barrier to trust and increases the likelihood of "automation bias" (accepting the AI's answer because it "sounds" confident and human-like).40

#### **3.2.2 Ontological Ambiguity**

AI occupies a liminal space between "object" and "subject." It is treated as a "cognitive partner," leading to confusion regarding authorship and responsibility. Is the AI a tool like a paintbrush, or a collaborator like a co-author? This **ontological ambiguity** is a key driver of the "loss of agency" and "creative displacement anxiety" experienced by users, who struggle to define where their work ends and the machine's begins.9

### **3.3 Opacity and Black Box Dynamics**

Unlike a bicycle (where the mechanics are visible) or a spreadsheet (where the formulas are auditable), the internal logic of the AI is opaque. The user cannot see *how* the answer was derived.

#### **3.3.1 Epistemic Opacity and Heuristics of Trust**

This lack of transparency forces the user to rely on **heuristics of trust** rather than verification of mechanism. Users might trust the AI because "it worked last time" or because "it uses professional language," rather than because they understand the derivation of the answer. This fosters a dependency based on faith in the system's output rather than understanding of the system's process, fundamentally undermining epistemic agency and making error detection difficult.43

## ---

**4\. Emerging Empirical Findings (2020-Present)**

Recent scholarship has moved from theoretical speculation to empirical observation of how humans actually interact with these systems. The data from 2020-2025 reveals a nuanced picture of productivity gains offset by cognitive risks.

### **4.1 Cognitive Offloading and Skill Atrophy**

The most robust finding in recent literature is the reality of skill atrophy due to cognitive offloading. The fears of the "Hollowed Mind" are finding empirical support.

#### **4.1.1 The Illusion of Competence**

Studies confirm that students and professionals using AI tools often produce higher-quality outputs but possess lower comprehension of the material compared to those who worked independently. Crucially, these users often attribute the success to their own "prompting" skills, masking a decline in critical thinking. This is the **Dunning-Kruger effect** amplified by algorithmic prosthetics: the AI masks the user's incompetence from themselves.45

#### **4.1.2 The Dose-Response Relationship**

Research indicates a negative correlation between the frequency of AI use and critical thinking scores. This relationship follows a **dose-response** pattern: the more frequent the reliance, the steeper the decline in independent analytical capability. The relationship is mediated by the intensity of cognitive offloading. "Moderate" use (3-4 times a week) appears to be a tipping point where dependency begins to set in, and the user starts to lose the "internal schemas" necessary to perform the task without assistance.45

#### **4.1.3 Writing Degradation and Schema Loss**

In controlled experiments, participants who relied on AI for brainstorming and structuring essays showed a diminished capacity to perform those tasks independently in subsequent trials. This suggests rapid **cognitive drift**, where the neural circuits for "structuring an argument" weaken through disuse. The AI acts as a crutch that, once removed, leaves the user less capable than before they started.45

### **4.2 Human-AI Co-Creativity and Agency**

The promise of "co-creativity" is complicated by the psychological struggle for ownership.

#### **4.2.1 Agency Fluctuations and the "IKEA Effect"**

Qualitative studies reveal that agency is not static during AI collaboration. It fluctuates. Users feel "high agency" during the prompting phase but "low agency" during the generation phase. If the AI's output is *too* good or complete, the user feels alienated from the result. This is the **IKEA Effect in reverse**: if I didn't build it (or at least assemble it), I don't value it. To regain agency, users often engage in "selective appropriation" or "counter-inspiration," rejecting the AI's first idea just to assert control.49

#### **4.2.2 The Curator Role**

Successful co-creators shift their identity from "author" to "curator" or "director." They view the AI as a subordinate engine or a "creative spar." However, novices struggle to maintain this hierarchy, often ceding creative control to the machine's suggestions because they lack the confidence or expertise to challenge the algorithmic output. This leads to a homogenization of creative work.49

#### **4.2.3 Flow State Facilitation**

Paradoxically, while agency may be threatened, some studies suggest AI *enhances* flow states. By removing the "blank page syndrome" and technical barriers (e.g., coding syntax or rendering skills), AI allows users to bypass the frustration phase and stay in the "zone" of high-level ideation. This supports the "Augmentation" hypothesis, but it appears restricted to those with sufficient meta-cognitive control to direct the flow rather than be carried by it.51

### **4.3 Social Dynamics in Hybrid Teams**

The impact of AI extends to team dynamics, creating new forms of social coordination.

#### **4.3.1 Convergence and Homogenization**

In group settings, AI agents tend to reduce the diversity of ideas. Teams using AI converge faster on solutions, but these solutions tend to be "standardized" or "median" responses. The AI acts as a harmonizing force that smoothes out the idiosyncratic (and often radical) ideas of human members. This "regression to the mean" poses a threat to radical innovation.7

#### **4.3.2 Hybrid Intelligence (Centaur) Performance**

Following Kasparov’s Centaur model, empirical data confirms that **Human \+ AI** generally outperforms *AI alone* or *Human alone* in strategic tasks, provided the human has the expertise to filter the AI's stochastic errors. This reinforces the **Expertise Duality**: you need expertise to use the tool that is supposed to replace expertise. The "Centaur" is only effective if the human half is a master, not a novice.54

## ---

**5\. Augmentation vs. Dependency: The Psychological Divide**

The central tension in the literature is distinguishing between healthy augmentation (extension) and pathological dependency (hollowing). Research is increasingly focused on identifying the boundary lines between these two states.

### **5.1 The Taxonomy of AI Dependence**

Researchers have developed psychometric scales, such as the **AIDep-22**, to categorize and measure the dimensions of this phenomenon. This taxonomy helps distinguish between using AI as a tool and needing AI as a crutch.45

| Dimension | Description | Psychological Mechanism | Maladaptive Outcome |
| :---- | :---- | :---- | :---- |
| **Functional Dependence** | Reliance on AI for task execution and speed. | Instrumental necessity; efficiency maximization. | Inability to perform tasks without the tool; "de-skilling." |
| **Cognitive Dependence** | Outsourcing of reasoning, decision making, and structuring. | Cognitive miserliness (conserving effort); lack of confidence. | "Hollowed Mind"; loss of critical thinking; "Epistemic Atrophy." |
| **Emotional Dependence** | Using AI for affect regulation; anxiety when AI is unavailable. | Anthropomorphism; social bonding; loneliness alleviation. | Social isolation; preference for AI over human interaction. |
| **Loss of Control** | Inability to regulate usage despite negative consequences. | Behavioral addiction; habit formation. | Compulsive usage; "tech-neck"; disruption of daily life. |

Table 2: Dimensions of AI Dependence derived from the AIDep-22 Scale.45

### **5.2 Cognitive Sovereignty and Epistemic Agency**

The antidote to dependency is the cultivation of **Cognitive Sovereignty**—the right and ability to govern one's own attention and beliefs.

#### **5.2.1 Epistemic Agency at Risk**

Epistemic agency is defined as the capacity to control one's own beliefs and knowledge acquisition. Uncritical AI use surrenders this agency. The user becomes a "data pipe," passing information from the AI to the world without verification, comprehension, or endorsement. This leads to a crisis of responsibility: if the AI wrote it, who is responsible for its truth? Preserving epistemic agency requires users to actively "warrant" the claims made by their tools, treating AI output as a *suggestion* to be verified rather than a *fact* to be consumed.57

#### **5.2.2 The Fortified Mind**

To maintain sovereignty, the user must possess a **Fortified Mind**—a core of foundational knowledge strong enough to audit the AI. This concept challenges the popular notion that "we don't need to learn facts anymore because we have Google/AI." On the contrary, the Fortified Mind hypothesis argues that factual knowledge is *more* critical now than ever, because it is the only defense against the AI's hallucinations. Without internal knowledge, the user cannot be a curator; they can only be a consumer.4

### **5.3 The Paradox of Friction**

A key theoretical insight is the role of friction in learning.

#### **5.3.1 Constructive Friction**

Learning requires effort, confusion, and the resolution of cognitive dissonance—a process known as **constructive friction**. AI is designed to be "frictionless." It provides immediate answers and smooth text. Therefore, unmanaged AI use eliminates the friction necessary for deep learning. To achieve healthy augmentation, users must artificially re-introduce "desirable difficulties." This might mean using AI to *critique* one's work rather than *do* the work, or engaging in "Socratic" dialogue with the AI rather than treating it as an oracle.60

## ---

**6\. Developmental Effects: The "AI Native"**

The impact of AI on developing minds (students, junior professionals) is distinct from its impact on established experts. This generational divide is shaping up to be a defining feature of the cognitive landscape.

### **6.1 The Novice-Expert Reversal**

Current research highlights a dangerous inversion in how AI benefits users based on their experience level.

* **Experts:** Use AI for "efficiency." They already possess the mental schemas to spot errors, structure arguments, and code logic. They use AI to speed up the "drudgery," freeing up energy for high-level strategy. They are "augmenting" a developed capability.  
* **Novices:** Use AI for "competence." They lack the schemas to evaluate the output. By offloading the "drudgery" of basic coding or writing (which is actually the *practice* required to build expertise), they fail to build the foundational neural circuits required to become experts. This threatens to create a "lost generation" of professionals who can manage AI workflows but cannot function independently or debug the system when it fails. They are "automating" a capability they never acquired.4

### **6.2 The Cyborg Identity and Self-Concept**

For younger generations, the distinction between "my idea" and "the AI's idea" is blurring, leading to a **Cyborg Identity**.

#### **6.2.1 Self-Efficacy and Fragility**

For the "AI Native," self-efficacy is increasingly tied to the ability to manipulate digital agents rather than internal skill. While this enables high performance at a young age, it creates psychological fragility. If the system fails, or if the "digital twin" is removed, the self-concept collapses because it was never grounded in internal capability. This creates a dependency that is existential, not just functional.63

#### **6.2.2 Creative Displacement Anxiety**

This entanglement also breeds **Creative Displacement Anxiety**. Young creatives worry that their unique contribution is being eroded by the machine. They face an identity crisis: "If the machine can paint/write/code better than I can, who am I?" This anxiety is driving some to reject the technology entirely, while others merge with it completely, viewing themselves as "conductors" of an algorithmic orchestra.65

## ---

**7\. Key Thinkers and Theoretical Contributions**

The following table maps the intellectual landscape of this domain, linking foundational theorists to contemporary researchers who are adapting these ideas for the AI age.

| Scholar(s) | Key Concept | Contribution to AI Discourse |
| :---- | :---- | :---- |
| **Clark & Chalmers** | *Extended Mind Thesis* | Established the philosophical basis for AI as a constitutive part of cognition, now expanded to "System 0".1 |
| **Edwin Hutchins** | *Distributed Cognition* | Provided the framework for analyzing AI as a team member in a sociotechnical system; updated to include AI as an "adaptive" agent.1 |
| **Lambros Malafouris** | *Material Engagement* | Reframed creativity as thinking *through* the material (AI) rather than imposing will upon it; the concept of "thinging".11 |
| **Daniel Wegner** | *Transactive Memory* | Explained the "Google Effect" and the dangers of assuming the network knows everything; updated to "AI Knowledge Collapse".14 |
| **Garry Kasparov** | *Centaur/Hybrid Intelligence* | Popularized the model of Human \+ AI \> Human, emphasizing process and strategy over raw calculation.56 |
| **Cecilia Heyes** | *Cognitive Gadgets* | (Contextual) Work on how cultural tools shape cognitive biology; relevant to how AI might rewire the "reading brain." |
| **T. Nguyen / M. G. Smith** | *Epistemic Bubbles / AI Attachment* | Contemporary researchers defining the risks of "epistemic bubbles" and emotional attachment to chatbots.58 |
| **Authors of AIDep-22** | *AI Dependence Scale* | Developing empirical metrics to measure the psychological dimensions of AI addiction and reliance.45 |

## ---

**8\. Theoretical Gaps and Future Directions**

Despite the explosion of literature, significant gaps remain that require urgent scholarly attention.

1. **Longitudinal Neuro-Cognitive Data:** Most current studies are behavioral (test scores, surveys). We lack fMRI or neurological studies confirming if "cognitive offloading" to LLMs produces the same physical hippocampal atrophy as GPS usage. Does the "writing brain" physically change after 5 years of using ChatGPT?  
2. **The "Middle-Skill" Trap:** Research focuses on novices (students) and experts. We know little about the "journeyman" phase. How does one transition from novice to expert if the "practice" phase is automated? We need models for "AI-Assisted Apprenticeship" that do not short-circuit learning.  
3. **Epistemic Resilience Training:** We lack pedagogical frameworks for teaching "AI Literacy" that goes beyond "how to prompt" and addresses "how to think while prompting." Developing curricula for "Calibrated Sovereignty" is a critical gap.  
4. **Social-Emotional Long-term Effects:** The impact of "Companion AI" on human-human attachment styles is theorized but not longitudinally observed. Will "perfect" AI companions make human relationships feel too "high-friction" and messy?.42

## ---

**9\. Conclusion**

The integration of AI into the human cognitive apparatus is not merely an upgrade in efficiency; it is a fundamental restructuring of the self. The literature from 2018-2025 paints a complex picture where **Augmentation** and **Atrophy** are coupled forces, inextricably linked by the user's habits of mind.

The "Extended Mind" is becoming the "Hollowed Mind" for those who fail to maintain **Cognitive Sovereignty**. The structural features of AI—generativity, stochasticity, and semantic fluency—create a "frictionless" environment that seduces the user into passivity. Unlike the calculator, which demanded we formulate the problem, Generative AI offers to formulate the problem, the solution, and the critique.

True cognitive partnership requires a deliberate re-introduction of agency. The future of human intelligence does not lie in competing with the machine's generative capacity, but in cultivating the distinctively human capabilities of **curation, verification, mission-locking, and systemic integration**. The Human-AI Centaur is a viable model for the future, but only if the human remains the rider, possessing the strength to direct the steed, rather than a passenger carried passively by the algorithm.

## ---

**10\. Annotated Bibliography**

1 The Cyborg Developer: Empirical Analysis of Cognitive Extension Through Human-AI Collaborative Programming (2024)

* *Relevance:* This study is pivotal for critiquing classic Distributed Cognition. It argues that Human-AI systems are *adaptive* rather than static. Unlike a map or compass, the AI agent actively negotiates the distribution of cognitive load, creating a "dynamic" system that requires new theoretical tools to analyze.

45 AI Dependence Scale (AIDep-22) (2025)

* *Relevance:* Provides a validated psychometric tool for measuring the dimensions of AI dependency (Emotional, Functional, Cognitive, Loss of Control). It offers the necessary empirical grounding to the often anecdotal "addiction" narrative, allowing researchers to quantify the "Hollowed Mind."

57 Epistemic Agency in the Age of AI (2025)

* *Relevance:* This work defines the central philosophical risk of AI: the loss of the capacity to be responsible for one's own beliefs. It shifts the conversation from "productivity" to "responsibility," arguing that education must focus on "Epistemic Agency" as a core competency.

45 Cognitive Offloading and Skill Atrophy (2025)

* *Relevance:* Synthesizes recent findings on how systematic offloading displaces internal processing. It provides the mechanism for the "Illusion of Competence," explaining why users feel smarter while becoming less capable.

11 How Things Shape the Mind: A Theory of Material Engagement (2013)

* *Relevance:* A foundational text that is being rediscovered in the AI era. It provides the framework for understanding how interacting with active tools (like AI) constitutes a form of *thinking through* materials, offering a non-dualistic way to view the human-machine partnership.

56 Centaur Hypothesis and Innovation (2022)

* *Relevance:* Revisits Kasparov's chess model to apply it to modern innovation processes. It provides the optimistic counter-narrative to the "atrophy" studies, arguing for a "symbiotic" approach where the combined unit achieves heights unreachable by biology alone.

5 The Extended Hollowed Mind (2025)

* *Relevance:* A critical theoretical paper that merges EMT with the "Hollowed Mind" hypothesis. It argues that without foundational knowledge ("The Fortified Mind"), extension becomes impossible because there is nothing to extend *from*. It is the essential counter-argument to "we don't need to memorize facts."

9 The Architecture of Cognitive Sovereignty (2025)

* *Relevance:* Moves the debate from "AI as tool" to "AI as cognitive partner." It introduces "Cognitive Sovereignty" as the goal of human development in an AI age, emphasizing the need for categorical reasoning to navigate stochastic outputs.

7 AI as a Social Forcefield (2024)

* *Relevance:* An empirical study showing how AI alters group dynamics. It validates the idea that AI is not just a tool but a "social" actor that can reduce epistemic diversity in teams, creating a "forcefield" that aligns human thinking to the algorithmic mean.

19 Writing as Technology (Historical)

* *Relevance:* Provides the necessary historical context to compare AI with previous cognitive technologies. It allows for a nuanced comparison between the "passive externalization" of writing and the "active generation" of AI.

#### **Works cited**

1. The Cyborg Developer: Empirical Analysis of Cognitive Extension Through Human-AI Collaborative Programming \- ResearchGate, accessed January 30, 2026, [https://www.researchgate.net/publication/399199305\_The\_Cyborg\_Developer\_Empirical\_Analysis\_of\_Cognitive\_Extension\_Through\_Human-AI\_Collaborative\_Programming](https://www.researchgate.net/publication/399199305_The_Cyborg_Developer_Empirical_Analysis_of_Cognitive_Extension_Through_Human-AI_Collaborative_Programming)  
2. (PDF) Extending Minds with Generative AI \- ResearchGate, accessed January 30, 2026, [https://www.researchgate.net/publication/391886406\_Extending\_Minds\_with\_Generative\_AI](https://www.researchgate.net/publication/391886406_Extending_Minds_with_Generative_AI)  
3. The Mind-Technology Problem in the Age of GenAI: Introduction to the Special Issue, accessed January 30, 2026, [https://www.tandfonline.com/doi/full/10.1080/02691728.2025.2574296?src=](https://www.tandfonline.com/doi/full/10.1080/02691728.2025.2574296?src)  
4. The extended hollowed mind: why foundational knowledge is indispensable in the age of AI, accessed January 30, 2026, [https://pmc.ncbi.nlm.nih.gov/articles/PMC12738859/](https://pmc.ncbi.nlm.nih.gov/articles/PMC12738859/)  
5. The extended hollowed mind: why foundational knowledge is indispensable in the age of AI, accessed January 30, 2026, [https://www.researchgate.net/publication/398569016\_The\_extended\_hollowed\_mind\_why\_foundational\_knowledge\_is\_indispensable\_in\_the\_age\_of\_AI](https://www.researchgate.net/publication/398569016_The_extended_hollowed_mind_why_foundational_knowledge_is_indispensable_in_the_age_of_AI)  
6. Group Intelligence: A Distributed Cognition Perspective \- Diva-Portal.org, accessed January 30, 2026, [http://www.diva-portal.org/smash/get/diva2:277055/FULLTEXT01.pdf](http://www.diva-portal.org/smash/get/diva2:277055/FULLTEXT01.pdf)  
7. AI's Social Forcefield: Reshaping Distributed Cognition in Human-AI Teams \- IDEAS/RePEc, accessed January 30, 2026, [https://ideas.repec.org/p/arx/papers/2407.17489.html](https://ideas.repec.org/p/arx/papers/2407.17489.html)  
8. AI's Social Forcefield: Reshaping Distributed Cognition in Human-AI Teams \- arXiv, accessed January 30, 2026, [https://arxiv.org/abs/2407.17489](https://arxiv.org/abs/2407.17489)  
9. The Architecture of Cognitive Sovereignty: From Knowledge Storage to Categorical Reasoning | by Carlos E. Perez | Intuition Machine | Medium, accessed January 30, 2026, [https://medium.com/intuitionmachine/the-architecture-of-cognitive-sovereignty-from-knowledge-storage-to-categorical-reasoning-67408c269a8e](https://medium.com/intuitionmachine/the-architecture-of-cognitive-sovereignty-from-knowledge-storage-to-categorical-reasoning-67408c269a8e)  
10. Hulett Theory of Recursive Spiral Intelligence — Spedster, accessed January 30, 2026, [https://www.spedsternow.com/new-page-151](https://www.spedsternow.com/new-page-151)  
11. How Things Shape the Mind: A Theory of Material Engagement | Books Gateway | MIT Press, accessed January 30, 2026, [https://direct.mit.edu/books/monograph/2994/How-Things-Shape-the-MindA-Theory-of-Material](https://direct.mit.edu/books/monograph/2994/How-Things-Shape-the-MindA-Theory-of-Material)  
12. Rethinking Human–AI Co-Creativity in Artistic Practice with Material Engagement Theory \- Tilburg University, accessed January 30, 2026, [https://repository.tilburguniversity.edu/bitstreams/dcb93a86-6049-4138-8f18-b604591e6de0/download](https://repository.tilburguniversity.edu/bitstreams/dcb93a86-6049-4138-8f18-b604591e6de0/download)  
13. How Artists Use AI as a Responsive Material for Art Creation \- Tilburg University Research Portal, accessed January 30, 2026, [https://research.tilburguniversity.edu/en/publications/how-artists-use-ai-as-a-responsive-material-for-art-creation/](https://research.tilburguniversity.edu/en/publications/how-artists-use-ai-as-a-responsive-material-for-art-creation/)  
14. Google Effect \- The Decision Lab, accessed January 30, 2026, [https://thedecisionlab.com/biases/google-effect](https://thedecisionlab.com/biases/google-effect)  
15. Google effects on memory: a meta-analytical review of the media effects of intensive Internet search behavior \- NIH, accessed January 30, 2026, [https://pmc.ncbi.nlm.nih.gov/articles/PMC10830778/](https://pmc.ncbi.nlm.nih.gov/articles/PMC10830778/)  
16. Cognitive AI is Changing Human Memory (Just Like Google Did) | ObjectiveMind.AI, accessed January 30, 2026, [https://www.objectivemind.ai/cognitive-ai-is-changing-human-memory-just-like-google-did](https://www.objectivemind.ai/cognitive-ai-is-changing-human-memory-just-like-google-did)  
17. People mistake the internet's knowledge for their own \- PNAS, accessed January 30, 2026, [https://www.pnas.org/doi/10.1073/pnas.2105061118](https://www.pnas.org/doi/10.1073/pnas.2105061118)  
18. Time and Power: The Will to Temporalize in Digital Culture | CUNY Academic Works, accessed January 30, 2026, [https://academicworks.cuny.edu/context/gc\_etds/article/5908/viewcontent/TIME\_AND\_POWER\_THE\_WILL\_TO\_TEMPORALIZE\_IN\_DIGITAL\_CULTURE\_by\_TALHA\_CAN\_I%CC%87S%CC%A7SEVENLER\_\_.pdf](https://academicworks.cuny.edu/context/gc_etds/article/5908/viewcontent/TIME_AND_POWER_THE_WILL_TO_TEMPORALIZE_IN_DIGITAL_CULTURE_by_TALHA_CAN_I%CC%87S%CC%A7SEVENLER__.pdf)  
19. The Uses of Writing in Microbusinesses Susan Marion Grief UCL Submitted for the Degree of Doctor of Philosophy (PhD), accessed January 30, 2026, [https://discovery.ucl.ac.uk/10071538/1/Grief\_10071538\_thesis\_sig-removed.pdf](https://discovery.ucl.ac.uk/10071538/1/Grief_10071538_thesis_sig-removed.pdf)  
20. John Benjamins Publishing Company \- Salikoko Mufwene, accessed January 30, 2026, [https://mufwene.uchicago.edu/publications/LANGUAGE%20AS%20TECHNOLOGY%202013.pdf](https://mufwene.uchicago.edu/publications/LANGUAGE%20AS%20TECHNOLOGY%202013.pdf)  
21. Calculator Use in Mathematics Instruction and Standardized Testing \- ERIC \- Department of Education, accessed January 30, 2026, [https://files.eric.ed.gov/fulltext/ED372919.pdf](https://files.eric.ed.gov/fulltext/ED372919.pdf)  
22. Calculator literacy \- SFU Summit, accessed January 30, 2026, [https://summit.sfu.ca/\_flysystem/fedora/sfu\_migrate/8590/b35143228.pdf](https://summit.sfu.ca/_flysystem/fedora/sfu_migrate/8590/b35143228.pdf)  
23. The Final Report of the National Mathematics Advisory Panel 2008 US Department of Education, accessed January 30, 2026, [https://files.eric.ed.gov/fulltext/ED500486.pdf](https://files.eric.ed.gov/fulltext/ED500486.pdf)  
24. A HISTORICAL ANALYSIS OF ATTITUDES TOWARD THE USE OF CALCULATORS IN JUNIOR HIGH AND HIGH SCHOOL MATH CLASSROOMS IN THE UNITED ST \- ERIC, accessed January 30, 2026, [https://files.eric.ed.gov/fulltext/ED525547.pdf](https://files.eric.ed.gov/fulltext/ED525547.pdf)  
25. People banning chatgpt without any reason in schools and colleges feels like the time scientific calculators were banned. \- Reddit, accessed January 30, 2026, [https://www.reddit.com/r/ChatGPT/comments/zuojkm/people\_banning\_chatgpt\_without\_any\_reason\_in/](https://www.reddit.com/r/ChatGPT/comments/zuojkm/people_banning_chatgpt_without_any_reason_in/)  
26. No Calculators | PDF \- Scribd, accessed January 30, 2026, [https://www.scribd.com/document/932533421/No-Calculators](https://www.scribd.com/document/932533421/No-Calculators)  
27. The role of the hippocampus in navigation is memory | Journal of Neurophysiology, accessed January 30, 2026, [https://journals.physiology.org/doi/10.1152/jn.00005.2017](https://journals.physiology.org/doi/10.1152/jn.00005.2017)  
28. 'Place cells' help guide freely swimming zebrafish larvae | The Transmitter, accessed January 30, 2026, [https://www.thetransmitter.org/spatial-cognition-and-navigation/place-cells-help-guide-freely-swimming-zebrafish-larvae/](https://www.thetransmitter.org/spatial-cognition-and-navigation/place-cells-help-guide-freely-swimming-zebrafish-larvae/)  
29. Improved Navigation Performance Through Memory Triggering Maps: A Neurocartographic Approach \- PMC \- PubMed Central, accessed January 30, 2026, [https://pmc.ncbi.nlm.nih.gov/articles/PMC11659358/](https://pmc.ncbi.nlm.nih.gov/articles/PMC11659358/)  
30. Neuro-Navigation: Hippocampus's Role in Spatial Cognition \- ScienceHolic, accessed January 30, 2026, [https://www.scienceholic.org/post/neuro-navigation-hippocampus-s-role-in-spatial-cognition](https://www.scienceholic.org/post/neuro-navigation-hippocampus-s-role-in-spatial-cognition)  
31. Full article: Individuals learning to drive solo before age 18 have superior spatial navigation ability compared with those who learn later \- Taylor & Francis, accessed January 30, 2026, [https://www.tandfonline.com/doi/full/10.1080/13875868.2024.2319735](https://www.tandfonline.com/doi/full/10.1080/13875868.2024.2319735)  
32. Everyone in my class is writing with AI... : r/PhD \- Reddit, accessed January 30, 2026, [https://www.reddit.com/r/PhD/comments/1pwlbso/everyone\_in\_my\_class\_is\_writing\_with\_ai/](https://www.reddit.com/r/PhD/comments/1pwlbso/everyone_in_my_class_is_writing_with_ai/)  
33. SPELLING AND GRAMMAR CHECKERS IMPROVE ESSAY QUALITY IN A AWT? 1 Checking It Twice \- ERIC, accessed January 30, 2026, [https://files.eric.ed.gov/fulltext/ED603838.pdf](https://files.eric.ed.gov/fulltext/ED603838.pdf)  
34. Use of Spellcheck in Text Production by College Students with Dyslexia | Journal of Writing Research, accessed January 30, 2026, [https://www.jowr.org/index.php/jowr/article/download/580/465](https://www.jowr.org/index.php/jowr/article/download/580/465)  
35. Does spelling and grammar feedback support high-quality writing and revision?, accessed January 30, 2026, [https://par.nsf.gov/servlets/purl/10344107](https://par.nsf.gov/servlets/purl/10344107)  
36. A generative spiking neural-network model of goal-directed behaviour and one-step planning | PLOS Computational Biology \- Research journals, accessed January 30, 2026, [https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007579](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007579)  
37. Psychologically Inspired Symbolic Cognitive Architectures \- Computer Science \- University of Oregon, accessed January 30, 2026, [https://www.cs.uoregon.edu/Reports/AREA-200503-Ludwig.pdf](https://www.cs.uoregon.edu/Reports/AREA-200503-Ludwig.pdf)  
38. A mathematical theory of relational generalization in transitive inference \- bioRxiv, accessed January 30, 2026, [https://www.biorxiv.org/content/10.1101/2023.08.22.554287v1.full-text](https://www.biorxiv.org/content/10.1101/2023.08.22.554287v1.full-text)  
39. Beyond Hallucination: Generative AI as a Catalyst for Human Creativity and Cognitive Evolution \- ICCK, accessed January 30, 2026, [https://www.icck.org/article/html/tetai.2025.657559](https://www.icck.org/article/html/tetai.2025.657559)  
40. Human-AI in affordance perspective: a study on ChatGPT users in the context of Indonesian users \- Frontiers, accessed January 30, 2026, [https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2025.1623029/full](https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2025.1623029/full)  
41. PACIS 2025 Proceedings: A Cognitive Heuristics Perspective on Augmented Human Agents: Disclosure and Communication Style in E-Commerce \- AIS eLibrary, accessed January 30, 2026, [https://aisel.aisnet.org/pacis2025/hci/hci/6/](https://aisel.aisnet.org/pacis2025/hci/hci/6/)  
42. The concept and formation pathways of human-smart object attachment: a case study of wearable smart objects \- Frontiers, accessed January 30, 2026, [https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2025.1642265/full](https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2025.1642265/full)  
43. Call for Papers: Epistemic Agency in the Age of AI \- TU Delft, accessed January 30, 2026, [https://www.tudelft.nl/en/2025/tbm/call-for-papers-epistemic-agency-in-the-age-of-ai](https://www.tudelft.nl/en/2025/tbm/call-for-papers-epistemic-agency-in-the-age-of-ai)  
44. Roles of Artificial Intelligence in Collaboration with Humans: Automation, Augmentation, and the Future of Work | Management Science \- PubsOnLine, accessed January 30, 2026, [https://pubsonline.informs.org/doi/10.1287/mnsc.2024.05684](https://pubsonline.informs.org/doi/10.1287/mnsc.2024.05684)  
45. Development and validation of the AI dependence scale ... \- Frontiers, accessed January 30, 2026, [https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1725393/full](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1725393/full)  
46. (PDF) Cognitive Off-Loading in the Age of AI: A Hierarchical Risk Model for Human-RAG Systems \- ResearchGate, accessed January 30, 2026, [https://www.researchgate.net/publication/393339053\_Cognitive\_Off-Loading\_in\_the\_Age\_of\_AI\_A\_Hierarchical\_Risk\_Model\_for\_Human-RAG\_Systems](https://www.researchgate.net/publication/393339053_Cognitive_Off-Loading_in_the_Age_of_AI_A_Hierarchical_Risk_Model_for_Human-RAG_Systems)  
47. Generative AI dependency: the emerging academic crisis and its impact on student performance—a case study of a university in Zimbabwe \- Taylor & Francis, accessed January 30, 2026, [https://www.tandfonline.com/doi/full/10.1080/2331186X.2025.2549787](https://www.tandfonline.com/doi/full/10.1080/2331186X.2025.2549787)  
48. (PDF) Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task \- ResearchGate, accessed January 30, 2026, [https://www.researchgate.net/publication/392560878\_Your\_Brain\_on\_ChatGPT\_Accumulation\_of\_Cognitive\_Debt\_when\_Using\_an\_AI\_Assistant\_for\_Essay\_Writing\_Task](https://www.researchgate.net/publication/392560878_Your_Brain_on_ChatGPT_Accumulation_of_Cognitive_Debt_when_Using_an_AI_Assistant_for_Essay_Writing_Task)  
49. Full article: Agency in Human-AI Collaboration for Image Generation and Creative Writing: Preliminary Insights from Think-Aloud Protocols \- Taylor & Francis, accessed January 30, 2026, [https://www.tandfonline.com/doi/full/10.1080/10400419.2025.2587803](https://www.tandfonline.com/doi/full/10.1080/10400419.2025.2587803)  
50. What Makes It Mine? Exploring Psychological Ownership over Human-AI Co-Creations, accessed January 30, 2026, [https://openreview.net/forum?id=nt50snfl2I](https://openreview.net/forum?id=nt50snfl2I)  
51. Exploring creativity in human–AI co-creation: a comparative study across design experience, accessed January 30, 2026, [https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2025.1672735/full](https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2025.1672735/full)  
52. View of Engaging with AI painting: exploring motivations and challenges in laypeople's creative information practices \- Publicera, accessed January 30, 2026, [https://publicera.kb.se/ir/article/view/55511/44443](https://publicera.kb.se/ir/article/view/55511/44443)  
53. Co-Writing with AI, on Human Terms: Aligning Research with User Demands Across the Writing Process \- arXiv, accessed January 30, 2026, [https://arxiv.org/html/2504.12488v2](https://arxiv.org/html/2504.12488v2)  
54. Defining human-AI teaming the human-centered way: a scoping review and network analysis \- Frontiers, accessed January 30, 2026, [https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2023.1250725/full](https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2023.1250725/full)  
55. Hybrid Intelligence Teams: A Theoretical Framework for Human–AI Collaboration in Knowledge Work \- Robert Eccles, accessed January 30, 2026, [https://roberteccles.com/wp-content/uploads/2025/12/Hybrid-Intelligence-Teams-V-3.0-PDF.pdf](https://roberteccles.com/wp-content/uploads/2025/12/Hybrid-Intelligence-Teams-V-3.0-PDF.pdf)  
56. How AI-Human Symbiotes May Reinvent Innovation and What the New Centaurs Will Mean for Cities \- SciRP.org, accessed January 30, 2026, [https://www.scirp.org/journal/paperinformation?paperid=115255](https://www.scirp.org/journal/paperinformation?paperid=115255)  
57. Epistemic authority and generative AI in learning spaces: rethinking knowledge in the algorithmic age \- Frontiers, accessed January 30, 2026, [https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2025.1647687/full](https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2025.1647687/full)  
58. Full article: AI and Epistemic Agency: How AI Influences Belief Revision and Its Normative Implications \- Taylor & Francis, accessed January 30, 2026, [https://www.tandfonline.com/doi/full/10.1080/02691728.2025.2466164](https://www.tandfonline.com/doi/full/10.1080/02691728.2025.2466164)  
59. (PDF) ARTIFICIAL INTELLIGENCE DAN SELF-REGULATED LEARNING: TELAAH EPISTEMOLOGIS PERSPEKTIF TEORI BARAT DAN FILSAFAT ISLAM \- ResearchGate, accessed January 30, 2026, [https://www.researchgate.net/publication/399698999\_ARTIFICIAL\_INTELLIGENCE\_DAN\_SELF-REGULATED\_LEARNING\_TELAAH\_EPISTEMOLOGIS\_PERSPEKTIF\_TEORI\_BARAT\_DAN\_FILSAFAT\_ISLAM](https://www.researchgate.net/publication/399698999_ARTIFICIAL_INTELLIGENCE_DAN_SELF-REGULATED_LEARNING_TELAAH_EPISTEMOLOGIS_PERSPEKTIF_TEORI_BARAT_DAN_FILSAFAT_ISLAM)  
60. Defining human-AI teaming the human-centered way: a scoping review and network analysis \- PubMed Central, accessed January 30, 2026, [https://pmc.ncbi.nlm.nih.gov/articles/PMC10570436/](https://pmc.ncbi.nlm.nih.gov/articles/PMC10570436/)  
61. Impact of Excessive AI Tool Usage on the Cognitive Abilities of Undergraduate Students, accessed January 30, 2026, [https://assajournal.com/index.php/36/article/download/699/1019](https://assajournal.com/index.php/36/article/download/699/1019)  
62. Protecting Human Cognition in the Age of AI \- arXiv, accessed January 30, 2026, [https://arxiv.org/html/2502.12447v1](https://arxiv.org/html/2502.12447v1)  
63. https://www.echetana.com/Vol.-10, No.-02, April-June 2025, pp. 141-145 141 Blurring Boundaries: Artificial Intelligence and the, accessed January 30, 2026, [https://www.echetana.com/wp-content/uploads/2025/08/39.-A-E-Meenakshi-Yadav-3.pdf](https://www.echetana.com/wp-content/uploads/2025/08/39.-A-E-Meenakshi-Yadav-3.pdf)  
64. Human Enhancement Technologies and Our Merger with Machines \- MDPI, accessed January 30, 2026, [https://mdpi-res.com/bookfiles/book/3859/Human\_Enhancement\_Technologies\_and\_Our\_Merger\_with\_Machines.pdf?v=1712797286](https://mdpi-res.com/bookfiles/book/3859/Human_Enhancement_Technologies_and_Our_Merger_with_Machines.pdf?v=1712797286)  
65. Generative Artificial Intelligence and the Emergence of Creative Displacement Anxiety: Review \- ResearchGate, accessed January 30, 2026, [https://www.researchgate.net/publication/374770327\_Generative\_Artificial\_Intelligence\_and\_the\_Emergence\_of\_Creative\_Displacement\_Anxiety\_Review](https://www.researchgate.net/publication/374770327_Generative_Artificial_Intelligence_and_the_Emergence_of_Creative_Displacement_Anxiety_Review)  
66. Generative Artificial Intelligence and the Emergence of Creative Displacement Anxiety \- Semantic Scholar, accessed January 30, 2026, [https://pdfs.semanticscholar.org/171c/ce88f744ec4780d2c6decee02f348012044c.pdf](https://pdfs.semanticscholar.org/171c/ce88f744ec4780d2c6decee02f348012044c.pdf)  
67. Cognitive Partnership Models: A Practical Framework for Strategic Thinking | Eden Digital, accessed January 30, 2026, [https://www.edendigital.io/posts/cognitive-partnership-models](https://www.edendigital.io/posts/cognitive-partnership-models)  
68. Can Generative AI Chatbots Emulate Human Connection? A Relationship Science Perspective \- PMC \- PubMed Central, accessed January 30, 2026, [https://pmc.ncbi.nlm.nih.gov/articles/PMC12575814/](https://pmc.ncbi.nlm.nih.gov/articles/PMC12575814/)

[image1]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACsAAAAXCAYAAACS5bYWAAABQklEQVR4Xu2VwUpCQRSGDy2MchXhKmjRI0RQUOAT6K6ncNk6CCQIMjdCyza1cSf4DIJuW/YAQSBtioLC9Ixzp07/nejMmEUwH/yL+c7o/OgdJUok/i97nCFnzOlzFj6Pf40B2Q63nDLMprQ452L9RPYFG8LNmwLnVawbZDv0hJti5LbHmYRyjELJI6cJ7o5sh1UnipnAYj6n4QSFEndeSbha5trCUZ2zKwXFlzVfXwybnFNwR2Q7nIHPYTa9oVTw7RsHcE+2xyIOJNdkNy3jQMFPlV0n20Fe/BzmouGz8xVbnlx5nEsIpsMFSskKKT52QdWTjse5aHngHKCUmD8BvFCXsNYw62Nww6mAy/XwXaYRCgWzlO1ydsDtZ3nnhT5+qjChxJY9pPzZLktu05pn6PLsNgUQWxbPlpkbsWUTicRfMgHzbVyMrD/LawAAAABJRU5ErkJggg==>

[image2]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAWCAYAAAD5Jg1dAAAAaklEQVR4XmNgGLpAHoj/owtiAyBFBBXuAuKvDAQUygDxBiB+w0BAIUwSr8K1QCwLZeNUKAzE+5D4WBXyAfERNDGsCj9ABf8C8U8ohgUPiH0CoRQTEBWOIEBQ4SwgPsuAUHgJiGegqBhRAACWLSUCcL9VLgAAAABJRU5ErkJggg==>