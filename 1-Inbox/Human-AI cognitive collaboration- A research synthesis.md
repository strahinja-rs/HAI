# Human-AI cognitive collaboration: A research synthesis

**Generative AI's integration into human thinking and creative work presents a fundamental paradox: these tools can enhance immediate performance while potentially undermining the cognitive and creative capacities that make humans effective in the first place.** This synthesis examines how existing theoretical frameworks illuminate—and fail to capture—the unique dynamics of thinking with AI, what historical precedents suggest about cognitive tool integration, and what emerging empirical evidence reveals about effects on reasoning, creativity, skill development, and identity. The central tension across all domains is whether AI use constitutes healthy augmentation that expands human capability or problematic dependency that hollows it out. Current evidence suggests the answer depends critically on the **type of cognitive engagement** maintained during AI use, not simply frequency of use.

---

## 1. Theoretical frameworks for understanding human-AI collaboration

### The extended mind thesis offers the most ambitious account of AI integration

Andy Clark and David Chalmers' 1998 paper "The Extended Mind" argues that cognitive processes extend beyond the brain into the environment when external tools meet certain conditions: reliable availability, automatic endorsement, and ready accessibility. Under this view, AI assistants could become genuine parts of our cognitive systems—not merely tools we use, but extensions of who we are as thinking beings.

Clark has recently embraced this implication, suggesting future personal AIs "will be intimate technologies that fall just short of becoming parts of my mind. I will be very much lost without it." The framework illuminates why users report feeling diminished when separated from AI assistants and raises ethical questions about interfering with someone's cognitive tools. However, the theory faces significant challenges when applied to generative AI. Critics like Adams and Aizawa (2010) argue it confuses causal influences on cognition with what actually constitutes cognition—"Why did the pencil think that 2+2=4? Because it was coupled to the mathematician." More fundamentally, **AI's generativity and apparent understanding create dynamics absent from the static notebooks in Clark and Chalmers' original thought experiments**.

### Distributed cognition reframes collaboration as a systems-level phenomenon

Edwin Hutchins' distributed cognition framework, developed through ethnographic study of naval ship navigation (*Cognition in the Wild*, 1995), positions cognition as spread across social groups, material artifacts, and time. Human-AI systems become cognitive systems larger than any individual, with emergent properties belonging to the system rather than its components.

Recent applications to AI (Jacobsen et al., 2025) identify three key concerns: reconfiguring team cognition around shared mental models, adapting AI memory systems to align with human cognition, and establishing AI as fallback operator during disruptions. However, Schmutz et al. (2024) found that adding AI teammates often **reduces coordination, communication, and trust**—suggesting the framework's assumptions about effective communication between system components may not hold for AI collaboration. Hutchins himself is currently exploring how "distributed cognition and cognitive ethnography meet generative artificial intelligence," acknowledging the need for theoretical updates.

### Cognitive offloading research provides the most empirically grounded framework

Risko and Gilbert's (2016) review in *Trends in Cognitive Sciences* defines cognitive offloading as "the use of physical action to alter the information processing requirements of a task so as to reduce cognitive demand." This framework has generated substantial empirical research directly applicable to AI use.

The evidence reveals a consistent cost-benefit tradeoff: offloading enhances immediate task performance but diminishes recall for offloaded information. Grinschgl, Papenmeier, and Meyerhoff (2021) found cognitive offloading accelerates task processing but "subsequently diminishes recall performance." Critically, people make value-based decisions about offloading through metacognitive processes—suggesting that **active metacognitive engagement may moderate negative effects**. The framework's limitation is its focus on description rather than mechanism: it captures *what* happens but not fully *why*, and it was developed primarily around simpler cognitive tools than generative AI.

### Vygotskian scaffolding illuminates AI's potential as learning support

Vygotsky's zone of proximal development (ZPD)—the distance between what learners can accomplish independently versus with guidance from a "more knowledgeable other"—offers a developmental lens on AI collaboration. Recent research suggests generative AI can fulfill criteria for scaffolding: providing personalized learning paths, immediate feedback, and facilitation of self-regulated learning.

A striking application appears in Araujo et al.'s (2022) *Human Arenas* paper "Scaffolding Human Champions," examining how AI operates within human experts' ZPD in chess and Go—"a phenomenon unique in human history" where machines exist in our champions' zone of proximal development. However, the framework's emphasis on fading—gradually removing support as mastery develops—raises questions about whether AI scaffolds are designed to become unnecessary. More fundamentally, **AI lacks the authentic relational qualities Vygotsky emphasized**. As MIT's Per Urlaub (2024) notes, "If teachers are not able to design such learning environments, they probably serve their students better by not using ChatGPT."

### Creativity research frameworks reveal tensions between tool and partner

Csikszentmihalyi's flow theory and systems model of creativity have been adapted for human-AI collaboration through frameworks like COFI (Co-Creative Framework for Interaction, Rezwana & Maher, 2021) and Haase and Pokutta's (2024) four-level model ranging from "Digital Pen" to "AI Co-Creator." These frameworks identify five irreducible design tensions: ambiguity vs. precision, control vs. serendipity, speed vs. reflection, individual vs. collective, and originality vs. remix.

Studies confirm AI can match or exceed average human creativity on standardized tests (Koivisto & Grassini, 2023), though the best humans still outperform AI on divergent thinking tasks. The deeper issue is **qualitative**: AI creativity emerges from computational recombination of statistical patterns while human creativity is rooted in embodied, subjective experience. Flow states require intrinsic motivation and autotelic experience—psychological states AI cannot model and may interfere with.

### Where existing frameworks fail to capture AI's novelty

Across all frameworks, several features of generative AI remain inadequately theorized:

- **Generativity**: Unlike calculators or notebooks, AI produces novel outputs, not just retrieves information
- **Apparent understanding**: AI responds contextually in ways that create interactional dynamics absent from static tools
- **Variable reliability**: Stochastic outputs create uncertainty unprecedented for cognitive tools
- **Capacity for deception**: AI can produce plausible but false outputs—a failure mode impossible for calculators or GPS
- **Speed and scale**: AI operates at speeds outstripping human cognitive patterns, potentially bypassing reflective processes

---

## 2. Historical precedents: What cognitive tool integration reveals

### Writing restructured rather than destroyed memory

Plato's *Phaedrus* articulated foundational anxieties about writing: it would "produce forgetfulness in the minds of those who learn to use it" and create "semblance of wisdom, not its reality." Walter Ong's *Orality and Literacy* (1982) confirmed that literacy did transform cognition—but through restructuring rather than simple loss. Oral thought is additive, aggregative, and situational; literate thought is subordinative, analytic, and abstract. **Memory shifted from rote memorization to "where to find" knowledge**—a precursor to the "Google effect" two millennia later. The transformation enabled abstract philosophical thinking, science, and extended logical argument that oral cultures could not sustain.

### Calculator research provides the clearest experimental evidence

Hembree and Dessart's 1986 meta-analysis synthesizing 79 research reports found that "use of calculators in concert with traditional mathematics instruction apparently improves the average student's basic skills with paper and pencil"—at most grade levels. The exception was Grade 4, where "sustained calculator use appears to hinder the development of basic skills in average students." The Education Endowment Foundation concluded calculators "promote fundamental knowledge such as number sense" and "higher-order thinking and reasoning needed for problem solving." **The pattern is cognitive redistribution**: mathematical education shifted toward problem-solving and conceptual understanding rather than rote computation. Some computational fluency decreased; algebraic reasoning gained emphasis.

### "Google effects on memory" established transactive memory with technology

Sparrow, Liu, and Wegner's 2011 *Science* paper demonstrated four experimental findings: people are primed to think about computers when facing difficult questions; they have lower recall for information itself but enhanced recall for *where to access* it; they remember statements believed to be erased but forget those believed to be saved; and they remember folder locations better than content. A 2024 meta-analysis pooling 22 studies (N=30,889) confirmed "moderate but statistically significant effect size" for these effects. The theoretical framing—technology as "transactive memory"—builds on Wegner's work on how groups distribute memory responsibilities. The internet became a cognitive partner.

### GPS navigation shows the clearest evidence for skill atrophy

Dahmani and Bohbot's 2020 *Scientific Reports* study provides the strongest longitudinal evidence for cognitive skill decline. Greater lifetime GPS experience correlates with worse spatial memory during self-guided navigation, and **three-year follow-up showed that greater GPS use predicted steeper decline in hippocampal-dependent spatial memory**. Critically, the researchers ruled out reverse causation: "Those who used GPS more did not do so because they felt they had a poor sense of direction, suggesting that extensive GPS use led to a decline in spatial memory rather than the other way around." The mechanism involves GPS promoting stimulus-response navigation (relying on caudate nucleus) rather than cognitive map formation (relying on hippocampus).

### The "ironies of automation" persist across all domains

Bainbridge's 1983 *Automatica* paper identified paradoxes that remain relevant: attempting to eliminate human error introduces designer errors; operators left to monitor systems are poorly suited for extended vigilance; operators expected to intervene in failures no longer practice the skills needed for intervention. Strauch's 2018 follow-up confirmed these ironies "still affect operator performance today." The automation paradox—more automation requires higher-order skills for oversight while reducing opportunities to practice them—applies directly to AI.

### Where AI analogies hold and break down

Strong analogies exist with historical precedents: cognitive offloading dynamics mirror search engines and GPS; transactive memory extension follows Sparrow's findings; automation ironies recur when AI handles routine work. However, **AI differs fundamentally in scope** (affecting multiple cognitive domains simultaneously rather than single functions), **generativity** (producing rather than storing/retrieving), **pace** (years rather than centuries for adoption), and **agency illusion** (appearing to understand in ways calculators cannot). The multi-domain impact may compound effects in ways single-function tools never did.

---

## 3. Empirical findings on cognition, creativity, and skill development

### The landmark MIT Media Lab study reveals reduced neural engagement

Kosmyna et al.'s 2025 "Your Brain on ChatGPT" study measured EEG activity across 54 participants over four months. LLM users showed **weakest neural connectivity** compared to brain-only and search engine groups, with brain connectivity "systematically scaled down with external support." The LLM group demonstrated lowest brain engagement across alpha, theta, and delta bands—associated with creativity, memory load, and semantic processing respectively. Users struggled to accurately recall their own essays, reported lowest sense of ownership, and increasingly copy-pasted content by session 3. When LLM users switched to brain-only conditions, they showed weaker neural connectivity and under-engagement. Essays were rated "largely soulless" by teacher evaluators. (Limitations: Small sample, preprint status, limited diversity)

### The Wharton RCT demonstrates the learning paradox most clearly

Bastani et al.'s randomized controlled trial with approximately 1,000 Turkish high school students represents the strongest experimental evidence on AI and learning. Students using standard ChatGPT-4 performed **48% better** than controls during assisted practice—but scored **17% worse** on subsequent unassisted exams. Students using a teacher-guided "GPT Tutor" with scaffolding (providing hints, not direct answers) performed **127% better** during practice and showed no exam decrement compared to controls. The implication is stark: **design matters enormously**. Standard AI use enhanced immediate performance while harming learning; appropriately scaffolded AI preserved benefits while mitigating harms. Students using standard ChatGPT were "overly optimistic about their abilities." (High confidence: well-designed RCT, substantial sample)

### Cross-sectional studies consistently find negative correlations with critical thinking

Gerlich's 2025 *Societies* study of 666 UK participants found significant negative correlation between AI tool usage and critical thinking (measured via Halpern Critical Thinking Assessment), with cognitive offloading strongly correlated with AI usage (r = +0.72) and inversely correlated with critical thinking (r = -0.75). **Younger participants (17-25) showed higher AI dependence and lower critical thinking scores**; higher education appeared protective. The Microsoft/CMU CHI 2025 study of 319 knowledge workers found higher confidence in GenAI associated with *less* critical thinking, while higher self-confidence was associated with *more* critical thinking. Both studies are cross-sectional, precluding causal inference, but the consistency across populations is notable.

### Creativity research reveals an individual-collective tradeoff

Doshi and Hauser's 2024 *Science Advances* RCT with approximately 300 writers found AI-assisted stories were rated 8% more novel and 9% more useful—but AI-enabled stories were more similar to each other. They describe this as "individual creativity at the risk of losing collective novelty"—a social dilemma where individual benefits may produce collective costs. Design research (Wadinambiarachchi et al., 2024) found AI-generated images induced **higher design fixation, lower fluency, variety, and originality** through "fixation displacement"—users shifting attention from original inspirations to AI outputs.

### Agency and authorship present complex psychological dynamics

Scientific Reports research (2024) on poetry writing found people were most creative when writing alone; creativity **decreased** when editing AI-generated content. The critical distinction was between co-creators (actively collaborating) and editors (modifying AI output)—only co-creation preserved creative self-efficacy. Kirk and Givi (2025) documented an "AI-authorship effect": AI-generated content is perceived as less authentic, resulting in moral disgust and reduced loyalty. This effect is attenuated when content is factual rather than emotional, when AI only edits rather than creates, or when consumers believe most content is already AI-written. Professional writers in qualitative research (ACM C&C 2025) report viewing AI relationships through human metaphors (assistant, collaborator) but finding these insufficient, and deliberately avoiding AI until putting in their own effort first.

### Expertise moderates effects substantially

Research consistently shows different patterns for experts versus novices. Experts welcome AI for repetitive information foraging while retaining control over complex synthesis; they maintain an "open frame"—ability to adapt, question, and evolve understanding. Novices show greater risk of overreliance and require different AI support to develop verification capabilities. Students' reliance patterns are predicted by programming self-efficacy and need for cognition (NFC)—higher NFC predicts lower overreliance. A "false-confidence loop" emerges: trust → acceptance → satisfaction → more trust → less scrutiny.

---

## 4. What makes AI structurally distinct from prior cognitive tools

### Generativity fundamentally changes the human-tool relationship

Prior cognitive tools—writing, calculators, search engines, GPS—retrieve, store, compute, or navigate existing information. Generative AI **produces novel content** that didn't previously exist. This shifts the human role from user to evaluator, from creator to curator. The INFORMS Information Systems Research (2024) study found GenAI "tremendously enhanced" ideation but was "substantially counterproductive" for implementation—precisely because designers moved from "creators" to "evaluators" of AI-generated content. This role transformation has no precedent in cognitive tool history.

### Apparent understanding creates unprecedented interactional dynamics

AI's capacity for contextual, seemingly understanding responses creates psychological dynamics absent from calculators or GPS. Users may attribute intentions, develop relationships, and extend trust appropriate for human partners but inappropriate for statistical pattern-matching systems. This "agency illusion" may be particularly consequential for metacognitive processes: users may fail to recognize that AI "understanding" operates through fundamentally different mechanisms than human comprehension.

### Variable reliability demands constant calibration

Unlike calculators (highly reliable within domain) or GPS (reliable with known failure modes), generative AI produces outputs of variable quality with uncertain reliability. Users must constantly calibrate trust, but automation bias research shows humans are poorly equipped for this task. Parasuraman and Manzey (2010) found automation reliance "cannot be prevented by training or instructions alone"—and AI's variable reliability may make appropriate calibration even more difficult than for traditional automation.

### The capacity for complete outputs enables skill-bypassing at scale

Prior tools augmented specific cognitive functions: calculators handled arithmetic, spell-check identified errors, GPS provided directions. Generative AI can produce complete essays, code, analyses, and creative works—potentially bypassing entire skill sets rather than augmenting specific functions. This is qualitatively different from tools that left the constructive work to humans while handling components.

### Speed enables bypassing of reflective processes

AI's speed—generating substantial outputs in seconds—may systematically bypass the slower, effortful processing associated with deep learning and memory consolidation. The "desirable difficulties" research (Bjork, 1994) shows that conditions making learning feel harder often produce better long-term retention. AI that makes cognitive work feel easy may undermine the productive struggle necessary for genuine learning and skill development.

---

## 5. Distinguishing augmentation from dependency

### Theoretical markers of the distinction

The fundamental distinction between healthy augmentation and problematic dependency appears to center on **whether the human maintains active cognitive engagement with substantive aspects of the task**. When AI removes productive struggle, takes over decision-making, and operates as a black box, dependency and skill atrophy result. When AI serves as a tool under human direction, provides feedback supporting learning, and leaves cognitive work to humans, augmentation occurs.

Engelbart's 1962 "Augmenting Human Intellect" framework emphasizes synergistic structuring, amplification rather than replacement, and the principle that even small changes to basic capabilities can propagate to major changes in effective intellect. The key distinction from automation: augmentation enhances problem-solving capabilities through synergistic human-technology relationships rather than substituting technology for human judgment.

Messeri and Crockett (2024, *Nature*) identify three "illusions of understanding" that indicate problematic dependency:

- **Illusion of explanatory depth**: believing deeper understanding exists than actually does
- **Illusion of exploratory breadth**: believing all possibilities have been considered
- **Illusion of objectivity**: failing to recognize bias embedded in AI tools

### Empirical markers distinguishing positive from negative trajectories

**Markers of skill development:**
- Active metacognitive engagement during AI use
- Verification behaviors and cross-checking of AI outputs
- Periodic practice without AI support
- "First try, then check" approach—attempting independently before using AI
- Capability transfers to unassisted contexts
- Understanding of mechanisms ("why"), not just outputs ("what")

**Markers of skill atrophy:**
- Cognitive processes delegated to AI without engagement
- AI as permanent crutch rather than fading scaffold
- Difficulty eliminated rather than productive struggle preserved
- Solution paralysis—inability to begin without AI assistance
- Performance dependent on AI presence
- Reduced cross-checking and verification

### The role of desirable difficulties and productive failure

Robert Bjork's research on "desirable difficulties" shows that retrieval practice, spacing, interleaving, and generation effects—all conditions that make learning feel harder—produce better long-term retention. Manu Kapur's "productive failure" research demonstrates that students who attempt problems without solutions before receiving instruction significantly outperform direct instruction groups (effect sizes 0.36-0.87). **The promise of generative AI could easily lure us into thinking that we now have tools to skip the struggling phase—but the struggle IS the learning mechanism.**

### Substitutive versus augmentative use patterns predict outcomes

Research increasingly identifies the substitutive-augmentative distinction as central. When AI use is **substitutive**—replacing human effort and cognitive engagement—deskilling results. When AI use is **augmentative**—supporting human effort while preserving engagement—skill maintenance or development can occur. Macnamara et al. (2024) found AI specifically accelerates skill decay beyond traditional automation because AI assistants "take over cognitive processes more comprehensively." Medical research documented measurable erosion of diagnostic skills after just three months of AI tool use.

The challenge is that **the path of least resistance leads to dependency**: cognitive offloading is natural, AI makes tasks feel easier, and negative effects may not be apparent until AI is unavailable.

---

## 6. Key researchers and research programs

### Philosophy and cognitive science foundations
- **Andy Clark** (University of Sussex/Edinburgh): Extended mind thesis, predictive processing
- **David Chalmers** (NYU): Philosophy of mind, extended cognition
- **Edwin Hutchins** (UC San Diego): Distributed cognition, cognitive ethnography

### Cognitive offloading and memory
- **Evan Risko** (University of Waterloo): Cognitive offloading mechanisms
- **Sam Gilbert** (University College London): Cognitive offloading costs and benefits
- **Betsy Sparrow** (Columbia): Google effects on memory, transactive memory

### Learning and skill development
- **Hamsa Bastani** (Wharton): AI and learning RCTs
- **Robert Bjork** (UCLA): Desirable difficulties, memory research
- **Manu Kapur** (ETH Zurich): Productive failure

### Creativity and human-AI collaboration
- **Anil Doshi and Oliver Hauser**: Creativity-diversity tradeoffs
- **Mika Koivisto and Simone Grassini**: Human vs. AI divergent thinking
- **Jennifer Haase and Sebastian Pokutta**: Human-AI co-creativity levels

### Automation and human factors
- **Raja Parasuraman** (George Mason, deceased): Automation complacency and bias
- **Dietrich Manzey** (TU Berlin): Human-automation interaction
- **Lisanne Bainbridge**: Ironies of automation

### Missing perspectives
Current research underrepresents: phenomenological approaches to AI-mediated experience; non-Western cultural contexts; professional populations (most studies use students); longitudinal designs tracking development over years; developmental psychology perspectives on AI effects across the lifespan; and sociological analyses of how AI reshapes creative and knowledge-work communities.

---

## 7. Gaps, tensions, and critical open questions

### Where current understanding is inadequate

**Longitudinal evidence is critically lacking.** Most studies are cross-sectional, unable to establish causation or track development over time. The GPS spatial memory research provides a rare longitudinal example—similar designs are urgently needed for cognitive and creative skills with AI.

**Phenomenology of AI-mediated experience is unexplored.** Almost no research examines the subjective experience of thinking with AI—whether flow states occur, how creative absorption is affected, what it feels like to create when uncertain about the human versus AI contribution. This is a major gap given the importance of intrinsic motivation and autotelic experience in creativity research.

**Transfer and generalization remain unclear.** Do skills developed with AI support transfer to unassisted contexts? Do skills maintained without AI transfer to AI-assisted contexts? The limited evidence (Bastani et al.) suggests standard AI use harms transfer, but scaffolded use may preserve it.

**Individual differences are undertheorized.** Who benefits from AI collaboration and who is harmed? Current research identifies some moderators (expertise, need for cognition, self-efficacy), but we lack comprehensive models of individual variation in AI collaboration outcomes.

### Most significant unresolved questions

1. **Is AI-induced cognitive change reversible?** If skills atrophy through AI dependence, can they be recovered through targeted practice? The GPS research suggests hippocampal changes accumulate over years—are similar effects occurring with cognitive skills?

2. **What is the minimum cognitive engagement necessary to prevent skill atrophy?** The Bastani GPT Tutor condition preserved learning outcomes—what were the active ingredients? How much productive struggle is sufficient?

3. **How do effects compound across developmental stages?** Are children using AI during critical learning periods at greater risk than adults with established skills? The evidence that younger users show higher dependence and greater cognitive impacts is concerning but not yet understood mechanistically.

4. **What happens to collective cognition?** If individuals increasingly think through AI intermediaries, what happens to the diversity of ideas in communities, organizations, and cultures? The creativity-diversity tradeoff identified by Doshi and Hauser may have implications far beyond individual creative work.

5. **Can we design AI that systematically supports rather than undermines human development?** The Bastani results suggest this is possible, but we lack clear principles for AI design that preserves human learning and agency across domains.

### Tensions requiring resolution

- **Individual benefit versus collective cost**: AI may enhance individual productivity while reducing collective diversity and innovation
- **Short-term performance versus long-term capability**: AI improves immediate task completion while potentially harming underlying skill development
- **Accessibility versus authenticity**: AI democratizes creative and cognitive tools while raising questions about the meaning of human achievement
- **Efficiency versus development**: The most efficient approach (AI doing the work) may be incompatible with human growth (humans doing the work)

---

## 8. Annotated bibliography: Key sources

### Foundational theoretical works

**Clark, A., & Chalmers, D. (1998). The extended mind. *Analysis*, 58(1), 7-19.**
The seminal paper arguing cognitive processes extend beyond the brain. Introduces the parity principle: if external processes function like internal ones, they should count as cognitive. Essential foundation for understanding AI as potential cognitive extension.

**Hutchins, E. (1995). *Cognition in the Wild*. MIT Press.**
Groundbreaking ethnography of distributed cognition in naval navigation. Establishes methodology and theoretical framework for understanding cognition as spread across people, tools, and environment. Essential for systems-level analysis of human-AI collaboration.

**Risko, E.F., & Gilbert, S.J. (2016). Cognitive offloading. *Trends in Cognitive Sciences*, 20, 676-688.**
Comprehensive review of cognitive offloading research. Establishes cost-benefit framework and metacognitive model. Most directly applicable existing framework for understanding AI-mediated cognition.

**Engelbart, D.C. (1962). Augmenting human intellect: A conceptual framework. Stanford Research Institute.**
Foundational vision of intelligence augmentation through technology. Establishes distinction between augmentation and automation, emphasizes synergistic human-tool relationships.

### Historical precedent research

**Ong, W. (1982). *Orality and Literacy: The Technologizing of the Word*. Methuen.**
Influential analysis of how writing transformed consciousness. Demonstrates cognitive restructuring rather than simple skill loss. Provides historical perspective on current AI anxieties.

**Sparrow, B., Liu, J., & Wegner, D.M. (2011). Google effects on memory: Cognitive consequences of having information at our fingertips. *Science*, 333, 776-778.**
Landmark experimental study demonstrating internet's effects on memory. Establishes "transactive memory" framework for technology relationships. Essential precedent for AI memory effects research.

**Dahmani, L., & Bohbot, V.D. (2020). Habitual use of GPS negatively impacts spatial memory during self-guided navigation. *Scientific Reports*, 10, 6310.**
Strongest longitudinal evidence for cognitive skill decline from tool use. Three-year follow-up design rules out reverse causation. Critical precedent suggesting AI effects may accumulate over time.

**Bainbridge, L. (1983). Ironies of automation. *Automatica*, 19(6), 775-779.**
Classic analysis of paradoxes in human-automation systems. Identifies skill degradation irony directly relevant to AI: operators expected to intervene in failures no longer practice needed skills.

**Parasuraman, R., & Manzey, D.H. (2010). Complacency and bias in human use of automation. *Human Factors*, 52(3), 381-410.**
Integrated model of automation complacency and bias. Establishes that these phenomena cannot be overcome through simple training. Essential for understanding AI overreliance risks.

### Empirical research on AI and cognition (2022-2025)

**Kosmyna, N., et al. (2025). Your brain on ChatGPT: Neural evidence of reduced engagement. arXiv:2506.08872.**
First neuroimaging study of AI effects on cognition. EEG evidence of reduced neural connectivity with LLM use. Important but requires replication (preprint, small sample).

**Bastani, H., et al. (2024). Generative AI can harm learning. SSRN Working Paper.**
Critical RCT demonstrating AI's paradoxical effects: enhanced practice performance (48%), reduced exam performance (17%). Shows scaffolded AI can mitigate harms. High-quality experimental evidence.

**Gerlich, M. (2025). AI tools in society: Impacts on cognitive offloading and the future of critical thinking. *Societies*, 15:6.**
Large survey (N=666) documenting negative correlation between AI use and critical thinking, mediated by cognitive offloading. Cross-sectional but important for age and education effects.

**Lee, S., et al. (2025). The impact of using AI on critical thinking. *CHI 2025*.**
Microsoft/CMU survey of 319 knowledge workers. Documents inverse relationship between AI confidence and critical thinking. Important for real-world work context.

### Empirical research on AI and creativity

**Doshi, A., & Hauser, O. (2024). Generative AI enhances individual creativity but reduces the collective diversity of novel content. *Science Advances*.**
RCT documenting creativity-diversity tradeoff. Individual stories improved but became more similar. Critical finding for understanding collective implications.

**Wadinambiarachchi, S., et al. (2024). Effects of generative AI on design fixation. *CHI 2024*.**
Experimental evidence that AI images increase design fixation and reduce originality. Introduces "fixation displacement" concept. Important for understanding creative process effects.

**Koivisto, M., & Grassini, S. (2023). Best humans still outperform AI in a creative divergent thinking task. *Scientific Reports*.**
Comparison of human (N=256) versus AI creative performance. AI exceeds average humans but not best humans. Important for calibrating AI's creative role.

### Skill development and dependency

**Macnamara, B.N., et al. (2024). Does using AI assistance accelerate skill decay? *Cognitive Research: Principles and Implications*, 9:46.**
Theoretical analysis distinguishing AI from traditional automation. Documents unawareness of skill decay during AI use. Important for understanding deskilling mechanisms.

**Bjork, R.A. (1994). Memory and metamemory considerations in the training of human beings. In J. Metcalfe & A. Shimamura (Eds.), *Metacognition*. MIT Press.**
Foundational work on desirable difficulties. Establishes that conditions making learning harder often improve retention. Essential framework for evaluating AI learning effects.

**Kapur, M., & Bielaczyc, K. (2012). Designing for productive failure. *Journal of the Learning Sciences*, 21(1), 45-83.**
Establishes productive failure framework. Meta-analysis shows significant learning benefits from struggle before instruction. Critical for understanding what AI assistance may bypass.

### Emerging frameworks for human-AI collaboration

**Messeri, L., & Crockett, M.J. (2024). Artificial intelligence and illusions of understanding in scientific research. *Nature*, 627, 49-58.**
Identifies three illusions of understanding created by AI use. Important theoretical framework for understanding problematic dependency.

**Haase, J., & Pokutta, S. (2024). Human-AI co-creativity. arXiv:2411.12527.**
Four-level framework from Digital Pen to AI Co-Creator. Provides useful taxonomy for analyzing human-AI creative relationships.

**Schmutz, J.B., et al. (2024). AI-teaming: Redefining collaboration. *Current Opinion in Psychology*, 58:101837.**
Review finding AI teammates often reduce coordination and trust. Important counterpoint to optimistic collaboration narratives.

### Additional essential sources

**Grinschgl, S., & Neubauer, A.C. (2022). Supporting cognition with modern technology: Distributed cognition today and in an AI-enhanced future. *Frontiers in AI*, 5:908261.**
Extends distributed cognition framework specifically to AI. Important for theoretical development.

**Hembree, R., & Dessart, D.J. (1986). Effects of hand-held calculators in precollege mathematics education. *Journal for Research in Mathematics Education*, 17(2), 83-99.**
Meta-analysis establishing calculator effects on mathematics learning. Essential historical precedent for AI effects research.

**Adams, F., & Aizawa, K. (2010). Defending the bounds of cognition. In R. Menary (Ed.), *The Extended Mind*. MIT Press.**
Key critique of extended mind thesis. Important for understanding limitations of cognitive extension frameworks applied to AI.

---

## Conclusion: The critical question ahead

This synthesis reveals a consistent pattern across theoretical frameworks, historical precedents, and emerging empirical evidence: **the effects of AI on human cognition and creativity depend critically on the type of engagement maintained, not simply the fact of AI use**. Historical tools like writing, calculators, and GPS restructured cognitive labor rather than eliminating it—but the restructuring had real consequences, including measurable skill changes over time.

Generative AI differs from precedent tools in ways that may amplify both benefits and risks: its generativity, apparent understanding, variable reliability, capacity for complete outputs, and processing speed create dynamics for which existing frameworks are only partially adequate. The empirical evidence—particularly Bastani's learning paradox and the MIT neural engagement findings—suggests that unscaffolded AI use may enhance immediate performance while undermining the cognitive capacities that make humans effective independent of tools.

The most promising finding is that **design matters**: appropriately scaffolded AI preserved learning outcomes where standard AI harmed them. This suggests the question is not whether to integrate AI into cognitive and creative work, but how to do so in ways that maintain the productive struggle, active engagement, and skill development that characterize genuine human flourishing. The path of least resistance leads toward dependency; the path toward augmentation requires deliberate design and conscious effort.

The stakes extend beyond individual capability to collective cognition. If AI enhances individual creative output while reducing collective diversity, if it improves immediate performance while eroding long-term skill, if it makes thinking easier while making thinkers weaker, the aggregate consequences for human culture and capability could be profound. Understanding these dynamics—and designing for healthy collaboration rather than problematic dependency—may be among the most important intellectual challenges of the coming decades.