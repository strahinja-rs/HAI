---
type: log
created: 2026-02-01
---

# Session: 2026-02-01

## Who
- Strahinja, 30, non-academic background (product management, SaaS, e-commerce, tech media, NGOs, startup accelerators, EU programs)
- Currently building apps (vibe coding)
- Insecure about academic credentials but brings real-world experience
- Wants personalized guidance, not generic academic tone

## What We're Doing
- Reading through foundational HAI field overviews from 1-Inbox
- Started with "Mapping the Human-AI Interaction Field" (Gemini research)
- Second file queued: "Interaction Design in Human-AI Interaction" (Claude research)

## Workflow Established
- Read in Obsidian, dump questions in `1-Inbox/Reading notes.md` (working surface, clear after processing)
- Bring question batches to Claude in terminal
- Keep own thoughts/reactions in reading notes (don't clear those)
- Create concept notes AFTER finishing a reading, not during
- Spaced repetition: decide later once concept library has mass

## Collaboration Rules (from CLAUDE.md)
- Claude: infrastructure, navigation, scaffolding, questions, editing
- Human: understanding, synthesis, judgment, voice, creation
- Do NOT synthesize or write original analysis for him

## Clarifications Covered
- Ontological transformation = field changed its fundamental nature, not just grew
- Recommender systems = "you might also like" features
- ML interpretability (pre-2022) = explaining decisions to users (UX concern)
- Mechanistic interpretability = understanding model internals (safety/science concern)
- High-velocity field = moves fast, papers outdated in months
- Industrial white papers = company-published research, not peer-reviewed

## Human Observations (keep these — original thinking)
- The three HAI epochs (Explainability → Generative → Agentic) stack rather than replace. Each phase inherits the problems of the previous one and adds new layers. Trust calibration doesn't disappear when agents arrive — it gets harder.
- Intent is bidirectional: human specifies intent to AI (intent specification), AI reads/infers intent from human (intent reading). The quality of the full loop — specification, reading, translation — determines interaction quality. Could frame as "intent exchange."
- Direct Manipulation → new paradigm feels "more wave, more indirect." Not cause-and-effect but negotiation. Output is probabilistic, not deterministic.
- **Key formulation:** "From working on the thing to working on the relationships between things that produce the thing." Also: "You're not writing anymore, you're designing the writers." The shift to agentic AI means working on relationships between things, not on the things themselves. Higher complexity demands holistic attention — sensing the system rather than controlling parts. McGilchrist's left/right hemisphere lens (parts vs. wholes, manipulation vs. relationship) may map onto Direct Manipulation → Intent Specification → Agentic Orchestration.
- **Concept candidate:** Friction Discernment — the meta-skill of choosing where to keep friction and where to eliminate it. Goes beyond UI cognitive forcing functions into a life skill for the automation age. Connects to: cognitive forcing functions (Sec 4.3), intent expression atrophy, over-reliance.
- "What you mean vs what you said" — the gap may be the gap between partially formed and fully formed intent. Connects to intent expression atrophy observation. Labeled for further exploration.
- **Seed (analysis-level):** "AI whisperers" — some cognitive dispositions (comfort with ambiguity, iterative refinement, loose intent-holding) may make certain people naturally better at human-AI interaction. Already visible in data: frontier workers at 95th percentile use AI qualitatively differently, not just more. Social implications of a new class stratification based on AI interaction aptitude. Arrival analogy: Louise Banks as the archetype. Open question: learnable skill or innate disposition?
- **Seed (analysis-level):** The interaction field — human-AI interaction as a single entity/field, not two separate entities exchanging messages. Total intelligence of the field is determined by both participants. Analogy from relational therapy: the session itself is an entity. Goes beyond "distributed agency" (still implies two separate actors) toward something more unified. Connects to: extended cognition (Clark & Chalmers), intersubjective field (therapy traditions), human-AI symbiosis (Paper 2). Open question: does this require consciousness on both sides to be real, or does it work with one conscious + one responsive participant?
- **Seed (analysis-level):** Technological foundations shape epistemology and culture. Deterministic tools → deterministic worldview (mechanistic, cause-and-effect). What happens when the foundational tools become probabilistic/stochastic? Does culture shift from certainty-seeking to ambiguity-tolerant? Historical parallels: clockwork→mechanism, computing→information processing, networks→decentralization, stochastic AI→? Connects to philosophy of technology (McLuhan, Postman, Yuk Hui).
- Verifiability irony: problems with clear, verifiable outputs are easiest to evaluate AND easiest for AI to solve alone. Problems needing human-AI collaboration (ambiguous, contextual) are hardest to evaluate. The evaluation gap is widest where it matters most.
- Evaluation has the same "thing → relationships" shift: you can't evaluate a dynamic relationship with a snapshot. Tools built for snapshots, measuring a process. This is the methodological crisis.
- **Key formulation:** "Synthetic personas are good for known problem spaces at scale, human research is irreplaceable for emerging experiences." Complementary tools, not replacements.
- Intent expression atrophy hypothesis: As AI gets better at reading intent from minimal input, users lose practice (and eventually capacity) for precise intent expression. Self-reinforcing cycle. Noticed in own behavior ("the AI will figure it out"). Related to cognitive offloading but distinct: offloading navigation loses a skill, but offloading intent expression may lose access to *what you actually want*. Position taken: expression is constitutive of intent, not just its output. If you can't express your intent, it's not fully yours — you don't know it. "Expression is the scene." Connects to: cognitive offloading, cognitive forcing functions (Sec 4.3), control-autonomy paradox (Sec 5.3), over-reliance.

## Concepts Created (Session 3)
- ✅ [[Mechanistic-Interpretability]] + [[User-Interpretability]] (split into two notes)
- ✅ [[Intent-Expression-Atrophy]]
- ✅ [[Friction-Discernment]]
- ✅ [[Interaction-Field]]
- Queued: AI Whisperers, Stochastic Epistemology — depending on how they develop

## Process Observation
- Voice dictation vs. typing: voice is the channel for first expression (truer, more connected to inner speech), keyboard is for sculpting/editing. Dictated concept descriptions were sharper than written Session 2 versions. Friction discernment in action — but open question: choosing the higher-fidelity channel, or rationalizing comfort? Be mindful.
- Meta-observation: studying friction discernment as theory while practicing it in the research process itself. The research subject and the research method are entangled.

## Reading List (surfaced from this session)

### Priority: Practitioner Guidelines
- **Microsoft HAX Toolkit** — https://www.microsoft.com/en-us/haxtoolkit/ — Industry standard for human-AI interaction design. 2025 updates on ambiguity, correction, error recovery.
- **Google PAIR Guidebook** — https://pair.withgoogle.com/guidebook/ — Mental models, explainability, onboarding for agentic capabilities.

### Books (lower priority)
- **Iain McGilchrist** — *The Master and His Emissary* — left/right hemisphere, attention to parts vs wholes. Arose from: your observation about holistic vs analytical modes in agentic interaction.
- **Marshall McLuhan** — *Understanding Media* ("the medium is the message") — how tools shape perception. Arose from: your question about technology shaping epistemology.
- **Neil Postman** — *Technopoly*, *Amusing Ourselves to Death* — cultural consequences of technological shifts. Arose from: same question.
- **Yuk Hui** — *The Question Concerning Technology in China*, *Recursivity and Contingency* — philosophy of technology, non-Western perspective on technological determinism. Arose from: same question.
- **Ben Shneiderman** — *Human-Centered AI* — the Direct Manipulation originator, argues for high automation + high control. Mentioned in the paper.
- **Andy Clark & David Chalmers** — "The Extended Mind" (1998) — cognition extends beyond the skull into tools and environment. Arose from: the interaction field observation.

### Watching
- **Arrival** (2016, Denis Villeneuve) — linguistic relativity, interfacing with non-human intelligence. Arose from: [[AI-Whisperers]] concept (Louise Banks as archetype).

## Open Loops

### Conference Decisions
- **Feb 12** — CHI Workshop position paper submission deadline (GenAICHI / W60 / W66). Decision needed: apply or skip.
- **Feb 13** — IUI Cyprus early bird registration deadline. Decision needed: attend or skip.
- **Mar 4** — CHI Barcelona early bird registration deadline. Decision needed: attend or skip. Verify Category I pricing.
- **~May 15** — HHAI Brussels early bird registration deadline. More time to decide.
- Before any ACM conference: buy ACM + SIGCHI membership ($130) to unlock member rates.

## Reading Progress
- Mapping the HAI Field: **Complete.** Read through all sections (1-9 + conclusion). Done Session 4.
- Interaction Design synthesis: Not started yet (queued as next read)

---

## Session 3 (Feb 1, afternoon)

### What We Did
- Retrieval exercise on 4 concept candidates from Session 2
- Created 5 concept notes: [[Mechanistic-Interpretability]], [[User-Interpretability]], [[Intent-Expression-Atrophy]], [[Friction-Discernment]], [[Interaction-Field]]
- Introduced Guidance app (`~/Projects/myProtocols/`) as real-world anchor — added to CLAUDE.md
- Moved 2 iCloud backlog captures to inbox (primary orality detection, AI critiques written with AI)

### Retrieval Results
- Intent Expression Atrophy: strong — extended the formulation ("externalizing self-knowledge, disintegrating rather than integrating")
- Friction Discernment: strong — gym analogy, Phaedrus reference, cognitive friction distinction all recalled
- Interaction Field: core solid, therapy analogy recalled, philosophical connections (extended mind, open questions) lighter
- User Interpretability: fuzzy — needed prompting to recover the definition
- Mechanistic Interpretability: solid

### Inbox State
- Unprocessed: 2 iCloud captures (primary orality, AI critiques observation), conferences 2026, 3 research syntheses, 2 epubs (McGilchrist, Shneiderman), Reading notes.md
- No items cleared this session (no reading processing occurred)

## Session 4 (Feb 1, evening)

### What We're Doing
- Continuing reading: "Mapping the Human-AI Interaction Field" — Section 3 (Modes of Interaction)

### Human Observations

**On Generative UI:** Skeptical — haven't seen it widely deployed. Fragments exist (Claude artifacts, ChatGPT canvas, Perplexity cards) but the full vision isn't the norm yet. Leave for later.

**Chat as substrate:** Chat isn't being replaced by other modes — it's the substrate other modes layer on top of. Claude Code = chat + terminal. Cursor = chat + editor. Even ambient clinical docs output a draft that gets edited.

**Defining chat — working through it:**
- Google search is NOT chat. Google is a middleman between me and the internet. You don't start a conversation with a search box.
- Ambient AI is NOT chat. It's passive, background. Chat is foreground, active.
- Turn-based interface ≠ chat. Chat is a subset of turn-based interfaces.
- Clicking buttons/options within chat = low-dimensional, reductive. Typing text = high-dimensional, expanding the contextual space.
- **Working definition: Chat = turn-based + unconstrained response space.** Both sides can, in principle, say anything on each turn. When you constrain responses (buttons, selection), you momentarily drop out of chat into GUI, even if it's embedded in a chat interface.

**Open thread:** Chat is the dominant HAI interaction mode right now. This definition matters for understanding what's actually new about human-AI interaction vs. what's inherited from human-human communication patterns.

**Agentic interaction patterns mix freely:** The paper's typology (directive, feedback loop, task iteration, delegation) aren't exclusive modes — they compose within a single session, sometimes within a single exchange. Example: feedback loop to refine a task, then delegation to execute it.

**Ambient AI — freeing capacity:** Ambient AI makes sense when there's an activity that matters, but you want to do something with its content later. It frees up capacity to do the activity by listening and storing context. Capturing is one use case; acting (e.g., flagging a missed symptom) goes beyond capturing.

**Embodied AI and joint attention — attention as trigger vs. always-on:**
- LLM attention is reactive — no input, no processing. For an embodied LLM to have "joint attention" with a human, it needs a script firing triggers continuously (e.g., every 100ms feeding sensory input). The "aliveness" is manufactured, not spontaneous.
- Open question: does the mechanism matter for the interaction, or only the phenomenological experience? If it *feels* like joint attention, is it joint attention for purposes of the [[Interaction-Field]]?
- Key distinction emerging: it may be more *extension* than *field*. Something synthetic and external that "extends qualia off me" — pulls out or amplifies my phenomenological experience rather than creating a genuine shared field. Not resolved yet.

**The human's lens determines the interaction:**
- Same LLM, different phenomenological experiences depending on what the human knows. A deep learning engineer interprets responses through technical knowledge; a non-technical user may experience the LLM as more "real" because they lack the technical dimension.
- This asymmetry already exists and will get harder to see as systems become more capable. Extrapolate to embodied AI: the experience of "joint attention" with a robot companion will differ radically based on whether the human understands the script firing underneath.
- Connects to: [[User-Interpretability]], the "AI Whisperers" seed (some people interact qualitatively differently).

**[[Friction-Discernment]] vs. Cognitive Forcing Functions — key distinction:**
- Friction Discernment = individual meta-skill (I choose where to keep friction for myself)
- Cognitive Forcing Functions (CFFs) = design pattern (a designer imposes friction on other users)
- They converge when the user is their own designer — e.g., Guidance app, where you design friction protocols for yourself
- In education, the student is NOT their own designer — the teacher/system decides where friction goes. Tension between what the user wants (frictionless answers) and what's good for the user (forced thinking). Classic paternalism question in design.
- Connection: **Ontological Design** (Anne-Marie Willis, Heidegger) — we design tools, tools design us back. CFFs don't just shape interfaces, they shape thinking patterns. Friction Discernment as self-directed skill = ontological design on yourself.

**Adjustable autonomy:** A system feature that lets users dial up or down the agent's independence, based on trust level and activity type. This is the *mechanism* that enables interaction patterns (directive, feedback loop, delegation) to mix freely within a single session — the dial that shifts between patterns.

**Anthropomorphism reframe — organic vs. synthetic is the wrong axis:**
- Started from two examples: Apple's Liquid Glass (aligning software with the material it's displayed on = true to the material) and Guidance's evolution from cold/machine-like to warmer/softer aesthetic.
- Key distinction: "organic" doesn't mean "human-mimicking" — it means adaptive, responsive, alive-feeling. A glass candle holder is warm not because it pretends to be wood but because it interacts with light in a living way.
- **Proposed reframe:** The real axis isn't machine-like vs. human-like. It's **rigid vs. responsive**. You can be transparently machine and still feel warm if you're responsive enough.
- **But:** Transparent machine-ness only works if the user can perceive the machine-ness. As models become more capable, fewer people will be able to distinguish. The engineer sees the machine; the non-technical user doesn't. So "warm but transparent" becomes invisible deception for users who can't see through the glass — and that group is growing.
- **Open design problem:** Can you design warmth that doesn't depend on deception? The anthropomorphism trap may be approached without anyone choosing it.
- Connects to: the engineer vs. non-technical user observation (same system, different phenomenological experience), [[User-Interpretability]]

### Concepts Created (Session 4)
- ✅ [[AI-Whisperers]] — cognitive disposition for qualitatively different HAI experience
- ✅ [[Stochastic-Epistemology]] — stochastic tools → stochastic worldview
- ✅ [[Cognitive-Forcing-Functions]] — designer-imposed friction (literature concept)
- ✅ [[Adjustable-Autonomy]] — dial for agent independence (literature concept)
- ✅ [[Anthropomorphism-in-HAI]] — the human-like vs. machine-like debate (literature concept)
- ✅ [[Warm-Transparency]] — keep the machine visible as machine while making it warm and responsive
- ✅ [[Turn-Based-Interface]] — any alternating-turn interaction pattern
- ✅ [[Chat-as-Interaction-Pattern]] — turn-based + unconstrained response space

### Concepts Updated
- ✅ [[Friction-Discernment]] — added CFF distinction, ontological design connection
- ✅ [[Interaction-Field]] — added embodiment test, extension-vs-field thread, human lens observation

### System Updates
- Synced TAGS.md (added original, domain/safety, domain/technical; consolidated domain tags)
- Updated concept template (removed redundant sections, added custom section encouragement)
- Standardized tags across all 5 existing concept notes
- Updated CLAUDE.md: System Awareness section, concept creation workflow, status-based retrieval protocol, reading notes vs. log distinction

### Clarifications
- Embodied AI models aren't always LLMs — robotics typically uses specialized models (vision, RL for motor control), sometimes with an LLM as planner/orchestrator on top.
- "Attention Is All You Need" (Vaswani et al., 2017) introduced the Transformer architecture. "Attention" there is a mathematical mechanism (learned weighting of input relevance), not phenomenological attention. Same word, different claims.
- **Heuristic prompts** = rule-of-thumb strategies in the prompt ("think step by step"). **Ensemble prompts** = running same question through multiple prompt strategies, combining results. Both outperform naive single-shot prompts in controlled studies.
- **Mental models users need about LLMs:** (1) They predict next tokens, not "know" things. (2) Stateless between conversations. (3) More/clearer context = better output because prediction space is more constrained. (4) Fluency ≠ accuracy — can be confidently wrong.
- **Superstitious prompting** = adding things that don't mechanically help but user believes they do (saying "please," adding "be very careful"). Cargo cult pattern.
- **Single Reformulated Prompting** = rephrasing the question with more context after an unsatisfying answer, treating the first attempt as info about what the model needs rather than a final answer. Vs. "Single Copy & Paste" = lazy one-shot.
- **IAI / graphical wrappers** = structured inputs that guide users into constructing better context without knowing prompting technique. Essentially context engineering via UI.

## Between Sessions
- Kindle: Reading McGilchrist's *The Master and His Emissary*
- Next session: retrieval exercise on all 13 seed concepts, then start "Interaction Design in Human-AI Interaction" synthesis
- Conference deadlines approaching: Feb 12 CHI workshops, Feb 13 IUI
- Watch: Arrival (2016)
